{
  
    
        "post0": {
            "title": "Semantic Representations",
            "content": "Introduction . Word Embeddings are used to represent words in a multi-dimensional vector form. A word wi in vocabulary V is represented in the form of a vector of n dimensions. These vectors are generated by unsupervised training on a large corpus of words to gain the semantic similarities between them. . Downloading the required vector file . import sys . !mkdir vectors # Download the different files using these commands. This may take a while !cd vectors &amp;&amp; curl -O http://magnitude.plasticity.ai/word2vec/light/GoogleNews-vectors-negative300.magnitude # !cd vectors &amp;&amp; curl -O http://magnitude.plasticity.ai/glove/light/glove.6B.50d.magnitude !cd vectors &amp;&amp; curl -O http://magnitude.plasticity.ai/elmo/light/elmo_2x1024_128_2048cnn_1xhighway_weights.magnitude . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 4016M 100 4016M 0 0 39.7M 0 0:01:40 0:01:40 --:--:-- 44.9M % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 51.9M 100 51.9M 0 0 43.4M 0 0:00:01 0:00:01 --:--:-- 43.4M . !ls !cd vectors &amp;&amp; ls . sample_data vectors elmo_2x1024_128_2048cnn_1xhighway_weights.magnitude GoogleNews-vectors-negative300.magnitude . Install the required libraries. Again this may take a while . !pip3 install torch numpy scikit-learn pandas transformers==3.1.0 seaborn matplotlib sentence_transformers . Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.6.0+cu101) Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5) Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.22.2.post1) Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.1.3) Collecting transformers==3.1.0 Downloading https://files.pythonhosted.org/packages/ae/05/c8c55b600308dc04e95100dc8ad8a244dd800fe75dfafcf1d6348c6f6209/transformers-3.1.0-py3-none-any.whl (884kB) |████████████████████████████████| 890kB 8.1MB/s Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (0.11.0) Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.2) Collecting sentence_transformers Downloading https://files.pythonhosted.org/packages/f4/fd/0190080aa0af78d7cd5874e4e8e85f0bed9967dd387cf05d760832b95da9/sentence-transformers-0.3.8.tar.gz (66kB) |████████████████████████████████| 71kB 9.9MB/s Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0) Requirement already satisfied: scipy&gt;=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.17.0) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (2.23.0) Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (20.4) Collecting sentencepiece!=0.1.92 Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB) |████████████████████████████████| 1.1MB 14.7MB/s Collecting tokenizers==0.8.1.rc2 Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB) |████████████████████████████████| 3.0MB 52.3MB/s Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (2019.12.20) Requirement already satisfied: dataclasses; python_version &lt; &#34;3.7&#34; in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (0.7) Requirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (4.41.1) Collecting sacremoses Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB) |████████████████████████████████| 890kB 51.7MB/s Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (3.0.12) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.2.0) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0) Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from sentence_transformers) (3.2.5) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas) (1.15.0) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers==3.1.0) (3.0.4) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers==3.1.0) (1.24.3) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers==3.1.0) (2020.6.20) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers==3.1.0) (2.10) Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers==3.1.0) (7.1.2) Building wheels for collected packages: sentence-transformers, sacremoses Building wheel for sentence-transformers (setup.py) ... done Created wheel for sentence-transformers: filename=sentence_transformers-0.3.8-cp36-none-any.whl size=101996 sha256=d88a5a734040b3392f98e3da764628774ab81a202c9ec9de5fa0ed0904c70db3 Stored in directory: /root/.cache/pip/wheels/27/ec/b3/d12cc8e4daf77846db6543033d3a5642f204c0320b15945647 Building wheel for sacremoses (setup.py) ... done Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=edeeea45ac8dc6966f84571553917635487c3d6152e4d40457476b9f95d14906 Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45 Successfully built sentence-transformers sacremoses Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers, sentence-transformers Successfully installed sacremoses-0.0.43 sentence-transformers-0.3.8 sentencepiece-0.1.94 tokenizers-0.8.1rc2 transformers-3.1.0 . import torch torch.__version__ . &#39;1.6.0+cu101&#39; . Since pymagnitude can not installed properly with pypi, we&#39;ll use another method for it. Install Magnitude on Google Colab . ! echo &quot;Installing Magnitude.... (please wait, can take a while)&quot; ! (curl https://raw.githubusercontent.com/plasticityai/magnitude/master/install-colab.sh | /bin/bash 1&gt;/dev/null 2&gt;/dev/null) ! echo &quot;Done installing Magnitude.&quot; . Installing Magnitude.... (please wait, can take a while) % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 137 100 137 0 0 1122 0 --:--:-- --:--:-- --:--:-- 1122 Done installing Magnitude. . Import some libraries . import pymagnitude as pym import numpy as np import seaborn as sns import pandas as pd from sklearn.metrics.pairwise import cosine_similarity import matplotlib.pyplot as plt . def similarity_between_docs(doc1, doc2, is_1d=False): if is_1d: v1 = np.reshape(doc1, (1, -1)) v2 = np.reshape(doc2, (1, -1)) else: d1 = np.mean(doc1, axis=0) d2 = np.mean(doc2, axis=0) v1 = np.reshape(d1, (1, -1)) v2 = np.reshape(d2, (1, -1)) return cosine_similarity(v1, v2)[0][0] def plot_1d_heatmap(vec, name): v = vec.reshape(1, -1) plt.figure(figsize=(20, 2)) sns.heatmap(v, cmap=&quot;YlGnBu&quot;).set_title(name) plt.rcParams.update({&quot;font.size&quot;: 22}) plt.show() return . Static Word Embeddings . Static WEs have fixed vector value for each word. They loose the contextual information . Google&#39;s Word2Vec | Stanford&#39;s GloVe | Facebook&#39;s fastText | Google&#39;s Word2Vec . # glove_vectors = pym.Magnitude(&quot;./vectors/glove.6B.50d.magnitude&quot;) w2v_vectors = pym.Magnitude(&quot;./vectors/GoogleNews-vectors-negative300.magnitude&quot;) . For using other vectors, download the pre-trained vectors from pymagnitude repo and put them in ./vectors folder . print(&quot;Vector Name: {} nTotal words: {} nDimension of each word: {}&quot;. format(&quot;Word2Vec&quot;, len(w2v_vectors), w2v_vectors.dim)) . Vector Name: Word2Vec Total words: 3000000 Dimension of each word: 300 . for i, (key, vec) in enumerate(w2v_vectors): if i == 1000: print(&quot;Index = {} Word: {} nVector Size: {} nVector: {}&quot;.format(i, key, vec.shape, vec)) break . Index = 1000 Word: Paul Vector Size: (300,) Vector: [ 1.211609e-01 4.324040e-02 -5.569700e-03 8.384690e-02 2.085200e-02 -1.558410e-02 -4.038700e-02 -9.965050e-02 -3.380210e-02 -2.205920e-02 -5.531260e-02 -2.688810e-02 7.155520e-02 8.340790e-02 7.133600e-03 3.533860e-02 1.167710e-01 4.324040e-02 7.901800e-02 8.296890e-02 -4.411840e-02 1.525490e-02 8.472480e-02 -6.277540e-02 3.923500e-03 -1.234660e-02 3.402160e-02 6.101940e-02 1.942530e-02 2.151050e-02 -4.828880e-02 2.030320e-02 3.292420e-02 -5.882450e-02 6.453130e-02 -2.611980e-02 6.063500e-03 1.439883e-01 -3.731400e-02 1.064550e-02 1.251120e-02 -2.074220e-02 -1.679130e-02 -7.243310e-02 -7.199420e-02 -2.359560e-02 1.920580e-02 -2.677830e-02 5.443460e-02 3.468010e-02 2.370540e-02 -5.679400e-03 -2.238840e-02 -9.526060e-02 -8.724900e-03 5.202020e-02 2.963170e-02 -9.877250e-02 3.380210e-02 -5.487360e-02 6.233640e-02 -1.190760e-02 -9.306560e-02 -4.192340e-02 -8.296890e-02 -8.428580e-02 -2.513210e-02 -6.936020e-02 -1.525490e-02 2.019350e-02 2.238840e-02 -1.255508e-01 2.293720e-02 -1.712060e-02 -3.270470e-02 2.118120e-02 -7.901800e-02 4.148440e-02 8.472480e-02 4.697180e-02 7.243310e-02 -3.819200e-02 7.989600e-02 -8.604180e-02 5.443460e-02 -4.960570e-02 -6.672630e-02 1.290627e-01 7.111620e-02 -6.101940e-02 6.628730e-02 6.008700e-03 8.428580e-02 -6.173300e-03 -2.919280e-02 6.392800e-03 -2.809530e-02 -4.609380e-02 -4.806930e-02 -9.306560e-02 -5.706850e-02 1.569380e-02 9.569950e-02 -1.799850e-02 1.295000e-04 -7.726200e-02 7.945700e-02 -1.591330e-02 3.577760e-02 6.760430e-02 -1.723030e-02 -2.826000e-03 -9.218760e-02 1.481590e-02 1.322450e-02 -1.646200e-03 -7.155520e-02 -3.841150e-02 3.797250e-02 6.453130e-02 -7.901800e-02 -3.566800e-03 -2.480290e-02 -1.633038e-01 -4.763030e-02 -9.921150e-02 -5.750750e-02 3.292420e-02 3.753350e-02 5.114220e-02 -3.753350e-02 -1.119421e-01 8.121300e-03 -9.306560e-02 4.148440e-02 5.624500e-03 -4.389900e-03 7.111620e-02 2.469310e-02 4.302090e-02 3.072900e-03 5.882450e-02 1.262090e-02 -5.662950e-02 -6.716530e-02 -2.387000e-03 1.022844e-01 -1.299407e-01 -7.550610e-02 2.030320e-02 2.480290e-02 -8.340790e-02 1.598200e-03 1.290627e-01 6.804330e-02 4.192340e-02 -2.622960e-02 4.784980e-02 -3.336310e-02 2.271770e-02 -1.262090e-02 3.907000e-02 1.132591e-01 -8.823670e-02 -7.737200e-03 3.336310e-02 -6.804330e-02 -2.579060e-02 2.831480e-02 -6.447600e-03 -4.016750e-02 2.601010e-02 2.052270e-02 -1.150151e-01 1.093082e-01 2.941220e-02 -1.810830e-02 2.260790e-02 -9.109000e-03 2.754650e-02 -6.277540e-02 -4.236240e-02 1.659900e-03 -5.377600e-03 -3.885050e-02 -3.928950e-02 -1.037110e-02 -8.604180e-02 7.989600e-02 -6.145840e-02 -6.540930e-02 4.280100e-03 -6.892120e-02 -2.831480e-02 5.679400e-03 5.070320e-02 6.189740e-02 5.421510e-02 -2.151050e-02 -2.370540e-02 5.882450e-02 8.735880e-02 3.621660e-02 5.421510e-02 1.141371e-01 2.194940e-02 5.092270e-02 1.255508e-01 -7.594510e-02 -2.513210e-02 2.096170e-02 -1.264288e-01 8.121290e-02 -4.455740e-02 -1.022844e-01 -7.331110e-02 -4.345990e-02 -2.721730e-02 6.760430e-02 -8.472480e-02 1.545240e-01 -8.472480e-02 1.141371e-01 -1.093082e-01 -1.668160e-02 -7.846900e-03 2.784800e-03 4.631330e-02 -2.633930e-02 -1.036013e-01 -3.928950e-02 1.887650e-02 -3.753350e-02 -3.621660e-02 6.804330e-02 -4.653280e-02 6.233640e-02 -8.604180e-02 1.037110e-02 -3.446060e-02 3.577760e-02 4.236240e-02 -8.560300e-03 -2.809530e-02 3.270470e-02 7.155520e-02 -2.976900e-03 -1.152350e-02 4.938620e-02 -1.395984e-01 -2.897330e-02 1.388300e-02 -5.750750e-02 3.533860e-02 4.960570e-02 4.675230e-02 7.243300e-03 -3.358260e-02 -1.152300e-03 3.775300e-02 5.750750e-02 5.202020e-02 7.792100e-03 2.831480e-02 4.148440e-02 6.101940e-02 4.993500e-03 -1.059060e-02 -1.481590e-02 -3.402160e-02 3.380210e-02 -3.468010e-02 -5.882450e-02 6.189740e-02 -5.882450e-02 -8.560280e-02 2.140070e-02 5.882450e-02 1.152300e-03 -5.289810e-02 -2.260790e-02 6.233640e-02 2.875380e-02 7.287210e-02 -6.447600e-03 1.141371e-01 1.613280e-02 -5.662950e-02 8.033490e-02 -2.249820e-02 5.953800e-03 -4.126490e-02 3.950900e-02 3.072920e-02 -6.540930e-02 -7.989600e-02 4.653280e-02 -8.999300e-03 -6.557400e-03 4.938600e-03] . print(w2v_vectors.query(&quot;dog&quot;)) # Get the vector using the index print(w2v_vectors[1000]) . [ 1.719810e-02 -7.493400e-03 -5.798200e-02 5.405100e-02 -2.833580e-02 1.924540e-02 1.965490e-02 -2.768070e-02 -5.159400e-03 -2.129280e-02 6.027510e-02 -1.421706e-01 -7.575300e-03 -5.568890e-02 -8.435200e-03 3.603400e-02 -6.682670e-02 5.339590e-02 -6.289580e-02 -4.029260e-02 5.208550e-02 -3.324960e-02 4.782700e-02 -5.503380e-02 -2.997380e-02 6.715430e-02 -5.012010e-02 1.074469e-01 1.100676e-01 8.189600e-03 -3.259440e-02 -2.751690e-02 -1.220240e-02 -2.882720e-02 -3.308580e-02 2.610400e-03 -4.504300e-03 1.768940e-02 4.979250e-02 1.120331e-01 5.568900e-03 -7.141290e-02 -5.057000e-03 1.760750e-02 -3.603400e-02 -2.981000e-02 8.353340e-02 -2.358590e-02 -5.364200e-03 2.538760e-02 -2.358590e-02 3.996500e-02 7.698180e-02 4.749900e-03 3.865470e-02 2.518300e-03 9.237810e-02 -8.189550e-02 9.958490e-02 1.171110e-02 8.124030e-02 4.553390e-02 4.782700e-02 5.896500e-03 9.827500e-03 -4.078400e-02 7.657200e-03 -1.596960e-02 -5.208550e-02 1.054400e-03 1.159640e-01 4.111150e-02 -6.551640e-02 2.718930e-02 -2.293070e-02 -4.934200e-03 7.206810e-02 -4.062020e-02 5.274070e-02 -6.944740e-02 4.586150e-02 -4.356840e-02 1.777130e-02 -9.106780e-02 -1.002401e-01 -6.191300e-02 -7.698180e-02 3.996500e-02 5.138900e-03 -8.779200e-02 -4.127530e-02 -6.265000e-03 -2.178420e-02 -2.735310e-02 2.637040e-02 -1.185847e-01 1.760750e-02 -8.230500e-03 -1.822200e-03 -7.010260e-02 -7.075770e-02 -6.027510e-02 8.124030e-02 8.648170e-02 4.618910e-02 -7.075770e-02 -7.288700e-03 -4.651660e-02 6.183100e-03 -4.156200e-03 -5.339590e-02 5.405100e-02 6.977500e-02 3.455990e-02 3.292200e-02 -2.293070e-02 -2.927800e-03 -9.696430e-02 -7.206810e-02 -3.832710e-02 -7.436110e-02 1.384030e-02 -1.048262e-01 -1.875410e-02 -3.275820e-02 1.949110e-02 -1.359470e-02 -5.830960e-02 5.503380e-02 -8.517130e-02 -5.175800e-02 -7.780100e-03 -7.993000e-02 6.961100e-03 -9.172300e-02 1.310300e-03 3.816330e-02 -5.830960e-02 8.648170e-02 7.894730e-02 1.752560e-02 2.293070e-02 -5.896480e-02 5.372350e-02 -2.009000e-04 2.006440e-02 -7.108530e-02 -1.859030e-02 -2.522380e-02 -1.028608e-01 1.434809e-01 1.785320e-02 -7.010260e-02 -1.916350e-02 -7.043010e-02 1.105590e-02 3.537890e-02 -5.044760e-02 -3.144790e-02 3.914610e-02 2.162040e-02 9.418000e-03 8.091280e-02 -4.225810e-02 -3.374090e-02 -4.115200e-03 -1.094000e-04 5.323200e-03 4.291320e-02 -1.113780e-02 1.367660e-02 -4.422360e-02 3.292200e-02 5.863720e-02 -7.927490e-02 1.736180e-02 6.158540e-02 8.148600e-03 -1.447913e-01 8.255070e-02 -1.019600e-02 -8.312400e-03 -3.930980e-02 5.405100e-02 -1.916350e-02 3.910500e-03 9.434360e-02 1.434809e-01 1.531450e-02 3.390470e-02 -1.326710e-02 5.937400e-03 -3.013750e-02 4.553390e-02 6.977500e-02 6.322330e-02 -5.110280e-02 -7.960240e-02 -6.387850e-02 2.391350e-02 -8.255070e-02 -8.779200e-02 -7.861970e-02 -4.880970e-02 -3.931000e-03 -5.044760e-02 -3.799950e-02 6.125780e-02 8.844720e-02 -4.618910e-02 -1.539636e-01 -1.572390e-02 -4.258570e-02 -1.416790e-02 -5.601650e-02 4.258570e-02 8.713680e-02 -8.189550e-02 -7.370600e-02 -2.915480e-02 5.339590e-02 -1.269380e-02 3.009700e-03 -9.303330e-02 -3.521510e-02 -5.896480e-02 7.665420e-02 -9.090400e-03 9.565400e-02 -9.172300e-02 5.405100e-03 1.981870e-02 -8.025760e-02 5.961990e-02 -4.520630e-02 4.651660e-02 1.185847e-01 4.094780e-02 4.815460e-02 3.095650e-02 7.698180e-02 -1.008953e-01 -1.637910e-02 -6.027510e-02 9.958490e-02 5.896480e-02 1.613340e-02 -1.136300e-03 2.653410e-02 -7.993000e-02 -7.763690e-02 5.568890e-02 -7.174050e-02 -2.358590e-02 -2.538760e-02 6.584400e-02 -4.356840e-02 -3.554270e-02 -1.185847e-01 -3.914610e-02 -1.711620e-02 1.138350e-02 -4.815460e-02 -1.310300e-03 5.830960e-02 -3.341340e-02 -5.568890e-02 -2.866340e-02 -1.284122e-01 1.981870e-02 -2.088340e-02 2.964620e-02 -2.981000e-02 1.100676e-01 2.293070e-02 -6.420610e-02 -2.802000e-04 3.488750e-02 5.110280e-02 -5.144000e-04 1.395500e-01 -1.113780e-02 5.012010e-02 8.124030e-02 -5.929230e-02 -1.654290e-02 -4.176670e-02 4.225810e-02 5.863720e-02 9.434360e-02 -6.060270e-02 3.455990e-02 -9.237810e-02 8.779200e-02 8.255070e-02 -1.580580e-02 2.096530e-02 1.395500e-01 -1.192399e-01 7.468870e-02] (&#39;Paul&#39;, array([ 1.211609e-01, 4.324040e-02, -5.569700e-03, 8.384690e-02, 2.085200e-02, -1.558410e-02, -4.038700e-02, -9.965050e-02, -3.380210e-02, -2.205920e-02, -5.531260e-02, -2.688810e-02, 7.155520e-02, 8.340790e-02, 7.133600e-03, 3.533860e-02, 1.167710e-01, 4.324040e-02, 7.901800e-02, 8.296890e-02, -4.411840e-02, 1.525490e-02, 8.472480e-02, -6.277540e-02, 3.923500e-03, -1.234660e-02, 3.402160e-02, 6.101940e-02, 1.942530e-02, 2.151050e-02, -4.828880e-02, 2.030320e-02, 3.292420e-02, -5.882450e-02, 6.453130e-02, -2.611980e-02, 6.063500e-03, 1.439883e-01, -3.731400e-02, 1.064550e-02, 1.251120e-02, -2.074220e-02, -1.679130e-02, -7.243310e-02, -7.199420e-02, -2.359560e-02, 1.920580e-02, -2.677830e-02, 5.443460e-02, 3.468010e-02, 2.370540e-02, -5.679400e-03, -2.238840e-02, -9.526060e-02, -8.724900e-03, 5.202020e-02, 2.963170e-02, -9.877250e-02, 3.380210e-02, -5.487360e-02, 6.233640e-02, -1.190760e-02, -9.306560e-02, -4.192340e-02, -8.296890e-02, -8.428580e-02, -2.513210e-02, -6.936020e-02, -1.525490e-02, 2.019350e-02, 2.238840e-02, -1.255508e-01, 2.293720e-02, -1.712060e-02, -3.270470e-02, 2.118120e-02, -7.901800e-02, 4.148440e-02, 8.472480e-02, 4.697180e-02, 7.243310e-02, -3.819200e-02, 7.989600e-02, -8.604180e-02, 5.443460e-02, -4.960570e-02, -6.672630e-02, 1.290627e-01, 7.111620e-02, -6.101940e-02, 6.628730e-02, 6.008700e-03, 8.428580e-02, -6.173300e-03, -2.919280e-02, 6.392800e-03, -2.809530e-02, -4.609380e-02, -4.806930e-02, -9.306560e-02, -5.706850e-02, 1.569380e-02, 9.569950e-02, -1.799850e-02, 1.295000e-04, -7.726200e-02, 7.945700e-02, -1.591330e-02, 3.577760e-02, 6.760430e-02, -1.723030e-02, -2.826000e-03, -9.218760e-02, 1.481590e-02, 1.322450e-02, -1.646200e-03, -7.155520e-02, -3.841150e-02, 3.797250e-02, 6.453130e-02, -7.901800e-02, -3.566800e-03, -2.480290e-02, -1.633038e-01, -4.763030e-02, -9.921150e-02, -5.750750e-02, 3.292420e-02, 3.753350e-02, 5.114220e-02, -3.753350e-02, -1.119421e-01, 8.121300e-03, -9.306560e-02, 4.148440e-02, 5.624500e-03, -4.389900e-03, 7.111620e-02, 2.469310e-02, 4.302090e-02, 3.072900e-03, 5.882450e-02, 1.262090e-02, -5.662950e-02, -6.716530e-02, -2.387000e-03, 1.022844e-01, -1.299407e-01, -7.550610e-02, 2.030320e-02, 2.480290e-02, -8.340790e-02, 1.598200e-03, 1.290627e-01, 6.804330e-02, 4.192340e-02, -2.622960e-02, 4.784980e-02, -3.336310e-02, 2.271770e-02, -1.262090e-02, 3.907000e-02, 1.132591e-01, -8.823670e-02, -7.737200e-03, 3.336310e-02, -6.804330e-02, -2.579060e-02, 2.831480e-02, -6.447600e-03, -4.016750e-02, 2.601010e-02, 2.052270e-02, -1.150151e-01, 1.093082e-01, 2.941220e-02, -1.810830e-02, 2.260790e-02, -9.109000e-03, 2.754650e-02, -6.277540e-02, -4.236240e-02, 1.659900e-03, -5.377600e-03, -3.885050e-02, -3.928950e-02, -1.037110e-02, -8.604180e-02, 7.989600e-02, -6.145840e-02, -6.540930e-02, 4.280100e-03, -6.892120e-02, -2.831480e-02, 5.679400e-03, 5.070320e-02, 6.189740e-02, 5.421510e-02, -2.151050e-02, -2.370540e-02, 5.882450e-02, 8.735880e-02, 3.621660e-02, 5.421510e-02, 1.141371e-01, 2.194940e-02, 5.092270e-02, 1.255508e-01, -7.594510e-02, -2.513210e-02, 2.096170e-02, -1.264288e-01, 8.121290e-02, -4.455740e-02, -1.022844e-01, -7.331110e-02, -4.345990e-02, -2.721730e-02, 6.760430e-02, -8.472480e-02, 1.545240e-01, -8.472480e-02, 1.141371e-01, -1.093082e-01, -1.668160e-02, -7.846900e-03, 2.784800e-03, 4.631330e-02, -2.633930e-02, -1.036013e-01, -3.928950e-02, 1.887650e-02, -3.753350e-02, -3.621660e-02, 6.804330e-02, -4.653280e-02, 6.233640e-02, -8.604180e-02, 1.037110e-02, -3.446060e-02, 3.577760e-02, 4.236240e-02, -8.560300e-03, -2.809530e-02, 3.270470e-02, 7.155520e-02, -2.976900e-03, -1.152350e-02, 4.938620e-02, -1.395984e-01, -2.897330e-02, 1.388300e-02, -5.750750e-02, 3.533860e-02, 4.960570e-02, 4.675230e-02, 7.243300e-03, -3.358260e-02, -1.152300e-03, 3.775300e-02, 5.750750e-02, 5.202020e-02, 7.792100e-03, 2.831480e-02, 4.148440e-02, 6.101940e-02, 4.993500e-03, -1.059060e-02, -1.481590e-02, -3.402160e-02, 3.380210e-02, -3.468010e-02, -5.882450e-02, 6.189740e-02, -5.882450e-02, -8.560280e-02, 2.140070e-02, 5.882450e-02, 1.152300e-03, -5.289810e-02, -2.260790e-02, 6.233640e-02, 2.875380e-02, 7.287210e-02, -6.447600e-03, 1.141371e-01, 1.613280e-02, -5.662950e-02, 8.033490e-02, -2.249820e-02, 5.953800e-03, -4.126490e-02, 3.950900e-02, 3.072920e-02, -6.540930e-02, -7.989600e-02, 4.653280e-02, -8.999300e-03, -6.557400e-03, 4.938600e-03], dtype=float32)) . doc_vecs = w2v_vectors.query([&quot;I&quot;, &quot;read&quot;, &quot;a&quot;, &quot;book&quot;]) doc_vecs.shape . (4, 300) . mul_doc_vecs = w2v_vectors.query([[&quot;I&quot;, &quot;read&quot;, &quot;a&quot;, &quot;book&quot;], [&quot;I&quot;, &quot;read&quot;, &quot;a&quot;, &quot;sports&quot;, &quot;magazine&quot;]]) mul_doc_vecs.shape . (2, 5, 300) . print(&quot;Similarity between &quot;Apple &quot; and &quot;Mango &quot;: {}&quot;. format(w2v_vectors.similarity(&quot;apple&quot;, &quot;mango&quot;))) print(&quot;Similarity between &quot;Apple &quot; and [ &quot;Mango &quot;, &quot;Orange &quot;]: {}&quot;. format(w2v_vectors.similarity(&quot;apple&quot;, [&quot;mango&quot;, &quot;orange&quot;]))) print(&quot;Most similar to &quot;Cat &quot; from [ &quot;Dog &quot;, &quot;Television &quot;, &quot;Laptop &quot;]: {}&quot;. format(w2v_vectors.most_similar_to_given(&quot;cat&quot;, [&quot;dog&quot;, &quot;television&quot;, &quot;laptop&quot;]))) . Similarity between &#34;Apple&#34; and &#34;Mango&#34;: 0.5751857161521912 Similarity between &#34;Apple&#34; and [&#34;Mango&#34;, &#34;Orange&#34;]: [0.5751857, 0.39203462] Most similar to &#34;Cat&#34; from [&#34;Dog&#34;, &#34;Television&#34;, &#34;Laptop&#34;]: dog . doc1 = w2v_vectors.query([&quot;I&quot;, &quot;read&quot;, &quot;a&quot;, &quot;book&quot;]) doc2 = w2v_vectors.query([&quot;I&quot;, &quot;read&quot;, &quot;a&quot;, &quot;sports&quot;, &quot;magazine&quot;]) print(&quot;Similarity between n &quot;I read a book &quot; and &quot;I read a sports magazine &quot;: {}&quot;. format(similarity_between_docs(doc1, doc2, is_1d=False))) . Similarity between &#34;I read a book&#34; and &#34;I read a sports magazine&#34;: 0.8234725594520569 . plot_1d_heatmap(w2v_vectors.query(&quot;king&quot;), &quot;King&quot;) plot_1d_heatmap(w2v_vectors.query(&quot;man&quot;), &quot;Man&quot;) plot_1d_heatmap(w2v_vectors.query(&quot;woman&quot;), &quot;Woman&quot;) plot_1d_heatmap(w2v_vectors.query(&quot;queen&quot;), &quot;Queen&quot;) tmp = w2v_vectors.query(&quot;king&quot;) - w2v_vectors.query(&quot;man&quot;) + w2v_vectors.query(&quot;woman&quot;) plot_1d_heatmap(tmp, &quot;King - Man + Woman&quot;) print(&quot;Similarity between n &quot;King - Man + Woman &quot; and &quot;Queen &quot;: {}&quot;. format(similarity_between_docs(tmp, w2v_vectors.query(&quot;queen&quot;), is_1d=True))) . Similarity between &#34;King - Man + Woman&#34; and &#34;Queen&#34;: 0.7118194103240967 . Exercises . It&#39;s time for you to explore these functionalities. . Try some to write code for some of the queries below. Or head on to this GitHub repository to have a look at different examples: . https://github.com/plasticityai/magnitude#querying . &quot;&quot;&quot; Try plotting heatmap for four different words, out of which three should have same property and should be different. For example, &quot;girl&quot;, &quot;boy&quot;, &quot;man&quot;, &quot;water &quot;&quot;&quot; # Your code here . &#39; nTry plotting heatmap for four different words, out of which three should have nsame property and should be different. For example, &#34;girl&#34;, &#34;boy&#34;, &#34;man&#34;, &#34;water n&#39; . &quot;&quot;&quot; Calculate the similarity between two words with similar sense and two with no similarity. For example, similarity between &quot;cat&quot; &amp; &quot;dog&quot; and similarity between &quot;apple&quot; and &quot;lion&quot; &quot;&quot;&quot; # Your code here . &#39; nCalculate the similarity between two words with similar sense and two with no nsimilarity. For example, similarity between &#34;cat&#34; &amp; &#34;dog&#34; and similarity between n&#34;apple&#34; and &#34;lion&#34; n&#39; . &quot;&quot;&quot; Print the similarity score of &quot;paris&quot; with following: &quot;delhi&quot;, &quot;vienna&quot;, &quot;london&quot;, france, &quot;laptop&quot; &quot;&quot;&quot; # Your code here . &#39; nPrint the similarity score of &#34;paris&#34; with following: n&#34;delhi&#34;, &#34;vienna&#34;, &#34;london&#34;, france, &#34;laptop&#34; n&#39; . Contextual Word Embeddings . These algoirtms also take the context of the word in some sentence while generating the Embeddings . AllenAI&#39;s ELMo | Google&#39;s BERT | AllenAI&#39;s ELMo . elmo_vecs = pym.Magnitude(&#39;./vectors/elmo_2x1024_128_2048cnn_1xhighway_weights.magnitude&#39;) . ELMo generates embedding of a word based on its context. So we need to provide a full sentence in order to get the embedding of some word. . sen1 = elmo_vecs.query([&quot;yes&quot;, &quot;they&quot;, &quot;are&quot;, &quot;right&quot;]) sen2 = elmo_vecs.query([&quot;go&quot;, &quot;to&quot;, &quot;your&quot;, &quot;right&quot;]) . /usr/local/lib/python3.6/dist-packages/pymagnitude/third_party/allennlp/nn/util.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor). index_range = sequence_lengths.new_tensor(torch.arange(0, len(sequence_lengths))) . . right1 = sen1[-1] right2 = sen2[-1] print(&quot;right from sentence 1: {} tright from sentence 2: {}&quot;.format(right1.shape, right2.shape)) . right from sentence 1: (768,) right from sentence 2: (768,) . plot_1d_heatmap(right1, name=&quot;ELMo vec for right in &quot;yes they are right &quot;&quot;) plot_1d_heatmap(right2, name=&quot;ELMo vec for right in &quot;go to your right &quot;&quot;) . print(&quot;Simialrity between &quot;right &quot; from sentence 1 &amp; 2: t{}&quot;. format(similarity_between_docs(right1, right2, is_1d=True))) print(&quot;Simialrity between &quot;right &quot; from sentence 1 only: t{}&quot;. format(similarity_between_docs(right1, right1, is_1d=True))) print(&quot;Simialrity between &quot;right &quot; from sentence 2 only: t{}&quot;. format(similarity_between_docs(right2, right2, is_1d=True))) . Simialrity between &#34;right&#34; from sentence 1 &amp; 2: 0.7283329963684082 Simialrity between &#34;right&#34; from sentence 1 only: 0.9999997019767761 Simialrity between &#34;right&#34; from sentence 2 only: 1.000000238418579 . Google&#39;s BERT . Since pymagnitude doesn&#39;t have support for BERT yet, we&#39;ll use the huggingface&#39;s transfomers library for this. . import torch import transformers . # this may take a while for first time model_class, tokenizer_class, pretrained_weights = (transformers.BertModel, transformers.BertTokenizer, &#39;bert-base-uncased&#39;) # Load pretrained model/tokenizer tokenizer = tokenizer_class.from_pretrained(pretrained_weights) model = model_class.from_pretrained(pretrained_weights) . . tokenized1 = tokenizer.encode(&quot;yes they are right&quot;, add_special_tokens=False) tokenized2 = tokenizer.encode(&quot;go to your right&quot;, add_special_tokens=False) print(tokenized1, tokenized2) # you can also get the full sentence using the token_ids print(tokenizer.decode(tokenized1)) print(tokenizer.decode(tokenized2)) . [2748, 2027, 2024, 2157] [2175, 2000, 2115, 2157] yes they are right go to your right . input_ids = torch.tensor([tokenized1, tokenized2]) model.eval() with torch.no_grad(): outputs = model(input_ids) last_hidden_states = outputs[0] . right1_bert = (last_hidden_states[0][-1]).numpy() right2_bert = (last_hidden_states[1][-1]).numpy() . print(right1_bert.shape, right2_bert.shape) . (768,) (768,) . plot_1d_heatmap(right1_bert, name=&quot;BERT vec for right in &quot;yes they are right &quot;&quot;) plot_1d_heatmap(right2_bert, name=&quot;BERT vec for right in &quot;go to your right &quot;&quot;) . print(&quot;Simialrity between &quot;right &quot; from sentence 1 &amp; 2 using BERT: t{}&quot;. format(similarity_between_docs(right1_bert, right2_bert, is_1d=True))) print(&quot;Simialrity between &quot;right &quot; from sentence 1 only using BERT: t{}&quot;. format(similarity_between_docs(right1_bert, right1_bert, is_1d=True))) print(&quot;Simialrity between &quot;right &quot; from sentence 2 only using BERT: t{}&quot;. format(similarity_between_docs(right2_bert, right2_bert, is_1d=True))) . Simialrity between &#34;right&#34; from sentence 1 &amp; 2 using BERT: 0.605478048324585 Simialrity between &#34;right&#34; from sentence 1 only using BERT: 0.9999997615814209 Simialrity between &#34;right&#34; from sentence 2 only using BERT: 0.9999997615814209 . Document Ranking . Let&#39;s use these embeddings to retrieve the documents based on a query . Load the data . !mkdir data !cd data &amp;&amp; curl -O https://raw.githubusercontent.com/ashishu007/Word-Embeddings/master/data/abstracts.csv !cd data &amp;&amp; curl -O https://raw.githubusercontent.com/ashishu007/Word-Embeddings/master/data/train.tsv . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 778k 100 778k 0 0 1569k 0 --:--:-- --:--:-- --:--:-- 1569k % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 703k 100 703k 0 0 3431k 0 --:--:-- --:--:-- --:--:-- 3431k . dfa = pd.read_csv(&#39;./data/abstracts.csv&#39;) print(dfa.shape) dfa = dfa[:50] dfa.head(5) . (792, 2) . title content . 0 Understanding Human Language: Can NLP and Deep... | There is a lot of overlap between the core pro... | . 1 Big Data in Climate: Opportunities and Challen... | This talk will present an overview of research... | . 2 A Sequential Decision Formulation of the Inter... | The Interface Card model is a promising new th... | . 3 Audio Features Affected by Music Expressivenes... | Within a Music Information Retrieval perspecti... | . 4 Automatic Identification and Contextual Reform... | Web search functionality is increasingly integ... | . Some utility functions . import nltk nltk.download(&#39;punkt&#39;) from nltk.stem import PorterStemmer from nltk.corpus import stopwords from sentence_transformers import SentenceTransformer . [nltk_data] Downloading package punkt to /root/nltk_data... [nltk_data] Unzipping tokenizers/punkt.zip. . torch.__version__ . &#39;1.6.0+cu101&#39; . def gen_w2v_embs(row): # row = row[&#39;text&#39;] # print(row) tokens = nltk.word_tokenize(row) token_words = [w for w in tokens if w.isalpha()] # print(token_words) stemming = PorterStemmer() tokens_stemmed = [stemming.stem(word) for word in token_words] # print(tokens_stemmed) # stops = set(stopwords.words(&quot;english&quot;)) stops = [&quot;a&quot;, &quot;an&quot;, &quot;the&quot;] meaningful_words = [w for w in tokens_stemmed if not w in stops] # print(meaningful_words) vecs = [] for w in meaningful_words: w_vec = w2v_vectors.query(w) vecs.append(w_vec) vec_arr = np.array(vecs) vec_final = np.mean(vec_arr, axis=0, dtype=&quot;float32&quot;) return vec_final . # Here we use a different library called `sentence_transformers` beacuse this # library is easier than `transformers` for sentence embeddings def gen_bert_embs(col): bert_model = SentenceTransformer(&#39;distilbert-base-nli-mean-tokens&#39;) bert_embs = bert_model.encode(col) return bert_embs . Embedding the documents with different algorithms . w2v_abs = dfa[&quot;content&quot;].apply(gen_w2v_embs) w2v_abs = (torch.tensor(w2v_abs)).numpy() . elmo_abs = dfa[&quot;content&quot;].apply((lambda x: elmo_vecs.query(x))) elmo_abs = (torch.tensor(elmo_abs)).numpy() . /usr/local/lib/python3.6/dist-packages/pymagnitude/third_party/allennlp/nn/util.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor). index_range = sequence_lengths.new_tensor(torch.arange(0, len(sequence_lengths))) . bert_abs = gen_bert_embs(dfa[&quot;content&quot;]) . 100%|██████████| 245M/245M [00:10&lt;00:00, 22.3MB/s] . w2v_abs.shape, elmo_abs.shape, bert_abs.shape . ((50, 300), (50, 768), (50, 768)) . type(w2v_abs), type(elmo_abs), type(bert_abs) . (numpy.ndarray, numpy.ndarray, numpy.ndarray) . Perform natural language queries . def gen_query_emb(q, emb=&quot;w2v&quot;): if emb == &quot;w2v&quot;: query_emb = gen_w2v_embs(q) elif emb == &quot;elmo&quot;: query_emb = elmo_vecs.query(q) elif emb == &quot;bert&quot;: query_bert = gen_bert_embs(q) query_emb = query_bert.reshape(-1) return query_emb . q1 = gen_query_emb(&quot;documents that discuss learning methods&quot;, emb=&quot;elmo&quot;) q2 = gen_query_emb(&quot;documents that discuss learning methods&quot;, emb=&quot;bert&quot;) q3 = gen_query_emb(&quot;documents that discuss learning methods&quot;, emb=&quot;w2v&quot;) . /usr/local/lib/python3.6/dist-packages/pymagnitude/third_party/allennlp/nn/util.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor). index_range = sequence_lengths.new_tensor(torch.arange(0, len(sequence_lengths))) . q1.shape, q2.shape, q3.shape . ((768,), (768,), (300,)) . Get the similarity between query and documents . def get_doc_similarity(q, docs): sims = {} for i, doc in enumerate(docs): sim_score = similarity_between_docs(q, doc, is_1d=True) sims[i] = sim_score sims_sorted = {k: v for k, v in sorted(sims.items(), key=lambda item: item[1], reverse=True)} return sims_sorted . s = get_doc_similarity(q2, bert_abs) # s . ss = list(s.keys())[:10] ss . [17, 0, 34, 23, 8, 13, 42, 15, 11, 2] . dfa[&quot;content&quot;][17] . &#39;Doc2Sent2Vec is an unsupervised approach to learn low-dimensional feature vector (or embedding) for a document. This embedding captures the semantics of the document and can be fed as input to machine learning algorithms to solve a myriad number of applications in the field of data mining and information retrieval. Some of these applications include document classification, retrieval, and ranking.&#39; . Exercises . &quot;&quot;&quot; Try ranking same documents using another natural language query and ranking algorithm &quot;&quot;&quot; # Your code here . &#39; nTry ranking documents using another natural language query and ranking algorithm n&#39; . &quot;&quot;&quot; Print top 3 documents ranked by each query and algorithm. Compare their results &quot;&quot;&quot; # Your code here . &#39; nPrint top 3 documents ranked by each query and algorithm. Compare their results n&#39; . &quot;&quot;&quot; Plot the heatmap for a sentence using BERT and ELMo &quot;&quot;&quot; # Your code here . &#39; nPlot heatmap for a sentence using BERT and ELMo n&#39; . &quot;&quot;&quot; Plot the heatmap for same sentence using Word2Vec &quot;&quot;&quot; # Your code here . Text Classification . Let&#39;s apply different embeddings for a simple Text Classification problem . Load the data . df = pd.read_csv(&#39;./data/train.tsv&#39;, delimiter=&#39; t&#39;, names=[&quot;text&quot;, &quot;label&quot;]) print(df.shape) df = df[:200] df.head(5) . (6920, 2) . text label . 0 a stirring , funny and finally transporting re... | 1 | . 1 apparently reassembled from the cutting room f... | 0 | . 2 they presume their audience wo n&#39;t sit still f... | 0 | . 3 this is a visually stunning rumination on love... | 1 | . 4 jonathan parker &#39;s bartleby should have been t... | 1 | . Embedding the data using Word2Vec . w2vs = df[&quot;text&quot;].apply(gen_w2v_embs) . w2v_embs = (torch.tensor(w2vs)).numpy() w2v_embs.shape . (200, 300) . Embedding the data using ELMo . elmos = df[&quot;text&quot;].apply((lambda x: elmo_vecs.query(x))) . /usr/local/lib/python3.6/dist-packages/pymagnitude/third_party/allennlp/nn/util.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor). index_range = sequence_lengths.new_tensor(torch.arange(0, len(sequence_lengths))) . elmo_embs = (torch.tensor(elmos)).numpy() elmo_embs.shape . (200, 768) . Embedding the data using BERT . bert_embs = gen_bert_embs(df[&quot;text&quot;]) . bert_embs.shape . (200, 768) . Prepare the features and labels for train and test . labels = df[&quot;label&quot;] . from sklearn.model_selection import train_test_split X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(w2v_embs, labels, test_size=0.33, random_state=42, stratify=labels) X_train_elmo, X_test_elmo, y_train_elmo, y_test_elmo = train_test_split(elmo_embs, labels, test_size=0.33, random_state=42, stratify=labels) X_train_bert, X_test_bert, y_train_bert, y_test_bert = train_test_split(bert_embs, labels, test_size=0.33, random_state=42, stratify=labels) . from sklearn.linear_model import LogisticRegression lr_clf_w2v = LogisticRegression() lr_clf_w2v.fit(X_train_w2v, y_train_w2v) lr_clf_elmo = LogisticRegression() lr_clf_elmo.fit(X_train_elmo, y_train_elmo) lr_clf_bert = LogisticRegression() lr_clf_bert.fit(X_train_bert, y_train_bert) . /usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) . LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=None, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0, warm_start=False) . y_pred_w2v = lr_clf_w2v.predict(X_test_w2v) y_pred_elmo = lr_clf_elmo.predict(X_test_elmo) y_pred_bert = lr_clf_bert.predict(X_test_bert) . from sklearn.metrics import accuracy_score, f1_score print(&quot;Word2Vec tAccuracy: {} tMacro F1: {}&quot;.format(accuracy_score(y_pred_w2v, y_test_w2v), f1_score(y_pred_w2v, y_test_w2v, average=&quot;macro&quot;))) print(&quot;ELMo tAccuracy: {} tMacro F1: {}&quot;.format(accuracy_score(y_pred_elmo, y_test_elmo), f1_score(y_pred_elmo, y_test_elmo, average=&quot;macro&quot;))) print(&quot;BERT tAccuracy: {} tMacro F1: {}&quot;.format(accuracy_score(y_pred_bert, y_test_bert), f1_score(y_pred_bert, y_test_bert, average=&quot;macro&quot;))) . Word2Vec Accuracy: 0.5757575757575758 Macro F1: 0.396078431372549 ELMo Accuracy: 0.5454545454545454 Macro F1: 0.5170731707317072 BERT Accuracy: 0.8333333333333334 Macro F1: 0.832371276841376 .",
            "url": "https://ashishu007.github.io/nlp-blog/jupyter/nlp/2020/12/07/Semantic_Representation.html",
            "relUrl": "/jupyter/nlp/2020/12/07/Semantic_Representation.html",
            "date": " • Dec 7, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Installing Elasticsearch and Kibana",
            "content": "Install Elasticsearch and Kibana on Windows . Since both ElasticSearch and Kibana is written in Java, its mandatory to have the current JDK installed. If not, go to this page and install the latest JDK. . Download and Unzip Packages . Download the Windows .zip packages for elasticsearch and kibana from the following links: Elasticseacrh: https://www.elastic.co/downloads/elasticsearch | Kibana: https://www.elastic.co/downloads/kibana | . | After downloading, unzip the files to some location. Let’s say Elasticseacrh: C: Users ashis Downloads elasticsearch-7.9.2-windows-x86_64 | Kibana: C: Users ashis Downloads kibana-7.9.2-windows-x86_64 | . | . Verify the JAVA_HOME path . Since both tools need JDK to run, we need to verify if the JAVA_HOME path is set correctly. For this follow these steps: . Under the search bar type env and click the Edit the System Environment Variables. | . Then click the Environment Variables option. | . Now, check if the value for JAVA_HOME is set to something like this: C: Program Files java jdk*. The one highlighted in blue is the path for JAVA_HOME: | . If JAVA_HOME isn’t setup, follow these setps: Copy the path for your JDK, it should be something like: C: Program Files Java jdk1.8.0_261 (depending on the version of Java installed in your system) | In the Environment Variables window, click New for System Variables | In the field Variable name write JAVA_HOME | In the field Variable value paste the JDK path that you’ve copied | . | . Running Elasticsearch . Open the command-prompt (type cmd in the search bar and click enter) and navigate to the folder where you unzipped elasticsearch. . | Run bin elasticsearch.bat . | Elasticsearch will start on that cmd. . Note: If you wish to explicitly define the cluster and node names, please use the following command: . bin elasticsearch.bat -Ecluster.name=my_cluster -Enode.name=my_node . When the cmd turns to this: | . Open http://localhost:9200/ in the browser. You should see something like this: | . Running Kibana . Open another command-prompt and navigate to the folder where you unzipped kibana. . | Run bin kibana.bat . | When cmd turns to this: . | . Open http://localhost:5601/ in the browser. You should see something like this: | . Experiments . Now everything is installed, goto this page for experimentation. .",
            "url": "https://ashishu007.github.io/nlp-blog/markdown/ir/elasticsearch/kibana/2020/11/23/install-es-kb.html",
            "relUrl": "/markdown/ir/elasticsearch/kibana/2020/11/23/install-es-kb.html",
            "date": " • Nov 23, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Elasticsearch and Kibana for Information Retrieval",
            "content": "Elasticsearch and Kibana for Information Retrieval . Objectives . The objectives of this lab are: . to explore the basic functionality of a state-of-the-art IR system; and | to compare the retrieval effectiveness of BM25. | . Approach . You will install and run Elasticsearch and employ a console available on Kibana to load and search our document collections (Documents and IRDocuments). . To install Elasticsearch and Kibana, goto this page. . Elasticsearch . In the previous labs we have used a simple stand-alone retrieval engine. This has been great to allow us to easily try different approaches to indexing. However, it is not suitable for real production applications because it is not optimised to scale for large volume data in a cloud computing, client-server architecture. There are 2 widely used, open-source IR systems that can scale to massive volume applications. These are Apache Solr (https://lucene.apache.org/solr/) and the more recent ElasticSearch (https://www.elastic.co). Both are built in Java, based on an IR engine called Lucene, and provide a server-side IR system that can be accessed via a REST API. ElasticSearch has become the industry standard for new applications. . ElasticSearch (ES) is a distributed, open source search and analytics engine for all types of data, including textual, numerical, geospatial, structured, and unstructured. Its speed and scalability along with the ability to index many types of content mean that it can be used for a wide range of applications. Core to its performance is an inverted index very similar to those we have already seen. . Kibana is a data visualization and management tool for Elasticsearch that provides real-time histograms, line graphs, pie charts, and maps. It also contains a simple console that we will use to interact with the ES API using JSON data format. . Install and Run ElasticSearch and Kibana . If you navigate to the Maths + Statistics folder in the AllPrograms section of your startup menu, you should find ElasticSearch and Kibana already downloaded and installed. . | Click first on the ElasticSearch folder to run the .bat file. You should see a command line interface open up as ElasticSearch starts up and runs in the background on a local server on your desktop. . | Go to http://localhost:9200/ and you should see something like this: . | . Now select the Kibana folder to run the .bat file. It should now open in the command line window. Point your browser at http://localhost:5601 and you should see something like this: | . On the top-left corner, click the three parallel lines (≡) to open the menu. Now scroll-down to find the Dev Tools option under Management section. Click it to go to the console where we will be writing the queries. . In a production system Elasticsearch would typically run on a cloud server with client side JavaScript applications passing information back and fore in JSON data format to the API made available by Elasticsearch. The Kibana console allows us to simulate sending requests (in the left-hand pane) and receiving answers (in the right-hand pane). This allows us to explore Elasticsearch without building the frontend application . ElasticSearch is open source and can be installed on your own machine. Check out this page for information on installing on Windows. . Add Individual Documents to the Index . First ensure that an Elasticsearch instance is running on http://localhost:9200 and a Kibana instance is running on http://localhost:5601. . As we have a clean Elasticsearch instance initialized and running, the first thing we are going to do is to add documents to an index and then retrieve them. Documents in Elasticsearch are represented in JSON format. . We are adding now to the index named irdocuments (the index name should be in lowercase only) a document having the id 1; since the index does not exist yet, Elasticsearch will automatically create it. . Run the following query in the Kibana console by clicking the green play-button: . POST irdocuments/_doc/1 { &quot;title&quot;: &quot;Document 1&quot;, &quot;content&quot;: &quot;The cat sat on the mat next to the cat.&quot; } . The response on the right should be something like this (if you copy and paste watch out for quotation mark errors): . . Verify that the document has been added trying: . GET irdocuments/_doc/1 . The response should be like this: . . Add the other 3 documents from IRDocuments and verify they have been added. You can try the following query to retrieve all the documents from the index irdocuments: . GET irdocuments/_search { &quot;query&quot;: { &quot;match_all&quot;: {} } } . Try: . GET irdocuments/_count . to count documents in the index. . Let’s delete document 1 and index a new doc with the text “The white cat sat on the mat next to the black cat.” Verify the result. . To delete a document from the index, run this query: . DELETE irdocuments/_doc/1 . Now you can delete the whole index by running: . DELETE irdocuments . Verify you have been successful. . Indexing a Set of Documents in Elasticsearch . Elasticsearch offers a Bulk API that allows us to perform add, delete, update and create operations in bulk for many documents at a time. Since the documents in Elasticsearch are represented in JSON format, we first need to convert our txt files into a predefined JSON format. This has already been done for us for our larger dataset with a python script (see Appendix-Py-script), the output of which is available as all_docs.json in lab resources on Moodle. . To get more insight for Bulk API please visit this link. . Let’s get this data into Elasticsearch. Since the body of these requests can be big, it is recommended to do this via a tool that allows to load the body of a request from a file - for instance, using curl. Open a command prompt and navigate to the folder where all_docs.json file is saved. Run the following command: . curl -H &quot;Content-Type: application/x-ndjson&quot; -XPOST &quot;localhost:9200/documents/_bulk?pretty&quot; --data-binary @all_docs.json . This command will index all these documents to an index named documents. Be patient, it can take a while to upload the json. Verify the indexing by running the following query: . GET irdocuments/_search { &quot;query&quot;: { &quot;match_all&quot;: {} } } . Compare the result here with the result you got previous time running this query (for individual documents). You’ll only get top 10 results from this query. . . Queries in Elasticsearch . Boolean . Let’s start with Boolean queries. In the boolean query, there are four kinds of occurrences: must, should, must_not and filter. must mimics the boolean “AND”. For example, if we are searching for documents with “case-base” in content and with “deep learning” in title, try the bool DSL query: . GET documents/_search { &quot;query&quot; : { &quot;bool&quot; : { &quot;must&quot;: [{ &quot;match&quot;: { &quot;content&quot;: &quot;case-based&quot; } }, { &quot;match&quot;: { &quot;title&quot;: &quot;deep learning&quot; } }] } } } . Try describing this query in Boolean Logic. . The functionality of should somewhat corresponds to the Boolean “OR”. Let’s look at an example, assume you want to search for either “NLP” or “deep learning” in the title, and if either of the criteria matches the document should be returned with the result. The query might look like: . GET documents/_search { &quot;query&quot; : { &quot;bool&quot; : { &quot;should&quot;: [{ &quot;match&quot;: { &quot;title&quot;: &quot;deep learning&quot; } }, { &quot;match&quot;: { &quot;title&quot;: &quot;nlp&quot; } }] } } } . Again try describing this query in Boolean Logic. . Try some queries using should and filter. Note filter can be used effectively with numeric data but is also quite similar to must, in that the term has to appear in the document; however, they do not contribute to the score. . Basic information on search is available at https://www.elastic.co/guide/en/elasticsearch/reference/7.0/getting-started-search.html . Ranked queries with BM25 . It is easy to perform ranked queries. The scores are based on the BM25 algorithm by default using the standard analyser (however, you can change to alternative models by setting up a custom analyser, see Appendix for more details). . For example, a ranked query for documents about “deep learning” can be written as: . GET documents/_search { &quot;query&quot;: {&quot;match&quot;: {&quot;content&quot;: &quot;deep learning&quot;}} } . Note the documents are ranked by score: . Try this query but replace “match” with “match_phrase”. Compare the results. | Now try one of the queries from the completed AnalysisHandout sheet from the coursework. Compare the results from tf, idf, and tf*idf with those obtained here. | . We have just touched the surface of the functionality available in ElasticSearch today. You will find more detailed information about Elasticsearch at this link. . Appendix-Analyzers . You can add different types of analyzers in your index in Elasticsearch. For example, applying a stop word removal on your index. . To define an analyser in an index, we need to first define the mappings here. For this, run the following command on Kibana console: . PUT documents/ { &quot;settings&quot;: { &quot;analysis&quot;: { &quot;analyzer&quot;: { &quot;my_analyzer&quot;: { &quot;type&quot;: &quot;stop&quot;, &quot;stopwords&quot;: &quot;_english_&quot; } } } }, &quot;mappings&quot;: { &quot;properties&quot;: { &quot;title&quot;: { &quot;type&quot;: &quot;text&quot; }, &quot;content&quot;: { &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;my_analyzer&quot; } } } } . Make sure there is no other existing index with name documnets. Now let’s add some data to the index documents. Open a command prompt, navigate to the folder where the JSON file name all_docs.json (from indexing example) is saved. Run the following command to add the JSON to our index. . curl -H &quot;Content-Type: application/x-ndjson&quot; -XPOST &quot;localhost:9200/documents/_bulk?pretty&quot; --data-binary @all_docs.json . Go to Kibana console and run following command to verify the indexing: . GET documents/_search { &quot;query&quot;: { &quot;match_all&quot;: {} } } . You should see the indexed files here. . Now to verify the stop-word removal from indexing run the following command: . GET /documents/_search { &quot;query&quot;: { &quot;match&quot;: { &quot;content&quot;: &quot;the&quot; } } } . You should see a result with 0 hits, because word “the” is not indexed here. . You can also customize the analyzers according to your use. Let’s say you want to apply an html filter as an analyser. You would need to create new mapping on different index. Changing the mapping of an existing field could invalidate data that’s already indexed. If you need to change the mapping of a field, create a new index with the correct mappings and re-index your data into that index. . Let’s make a different index named irdocuments. Run the following command: . PUT irdocuments/ { &quot;settings&quot;: { &quot;analysis&quot;: { &quot;analyzer&quot;: { &quot;blogs_analyzer&quot;: { &quot;type&quot;: &quot;custom&quot;, &quot;tokenizer&quot;: &quot;standard&quot;, &quot;stopwords&quot;: &quot;_english_&quot;, &quot;char_filter&quot;: &quot;html_strip&quot;, &quot;filter&quot;: [ &quot;stop&quot;, &quot;lowercase&quot;, &quot;asciifolding&quot; ] } } } }, &quot;mappings&quot;: { &quot;properties&quot;: { &quot;title&quot;: { &quot;type&quot;: &quot;text&quot; }, &quot;content&quot;: { &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;blogs_analyzer&quot; } } } } . Let’s run an example query to verify our analyser: . POST irdocuments/_analyze { &quot;analyzer&quot;: &quot;blogs_analyzer&quot;, &quot;text&quot;: &quot;is this the &lt;b&gt;déjà vu&lt;/b&gt;?.&quot; } . The result should show only two words: [deja, vu] . . Appendix-Py-Script . import json, os, re txt_files = &quot;C: Users User Downloads CourseworkResourses CourseworkResourses Documents &quot; ctr = 1 j = [] for txt in os.listdir(txt_files): s1 = &quot;&quot; s2 = &quot;&quot; print(ctr) f = open(txt_files + txt, &quot;r&quot;) l = f.readlines() lines = list(filter(lambda x: not x.isspace(), l)) t = lines[2] tt = t.strip(&#39; n&#39;) s1 = &quot;{ &quot;index &quot;:{ &quot;_id &quot;:&quot; + &quot; &quot;&quot; + str(ctr) + &quot; &quot;&quot; + &quot;}}&quot; ctr += 1 for i in lines: if i[0] == &quot;#&quot;: ii = i.replace(&#39;&quot;&#39;, &#39;&#39;) iii = ii.strip(&#39; n&#39;) iv = iii.replace(&#39;#&#39;, &#39;&#39;) s2 = &quot;{ &quot;title &quot;: &quot;&quot; + tt + &quot; &quot;, &quot;content &quot;:&quot; + &quot; &quot;&quot; + str(iv) + &quot; &quot;&quot; + &quot;}&quot; with open(&quot;all_docs.json&quot;, &quot;a&quot;) as the_file: the_file.write(s1 + &quot; n&quot;) the_file.write(s2 + &quot; n&quot;) .",
            "url": "https://ashishu007.github.io/nlp-blog/markdown/ir/elasticsearch/kibana/2020/11/23/es-kb.html",
            "relUrl": "/markdown/ir/elasticsearch/kibana/2020/11/23/es-kb.html",
            "date": " • Nov 23, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Machine Learning for Business Processes",
            "content": "Machine Learning for Business Processes . On 28th August 2020, my alma-mater, IIIT Naya Raipur (IIITNR) organised 3rd Annual Industry Academia Meet (IAM). This year, they introduced a special session on Carrer Guidance from Alumni for the current students. I feel honoured for being invited to talk about my research and guide other students to make career in Machine Learning. In this blog I’ll give a brief overview of the talk and share the slides. . . IAM at IIITNR is a great intiative started in 2017 where different personalities from IT Indsustry are invited to interact with the students of IIITNR. They deliver special lectures, talk about their work in the industry, judge different student competitions, and most importantly - guide the students on being industry-ready. . Introduction . Since, this event was all about making students industry-ready and guiding them for their career, decided to choose something related to real-world Machine Learning. The topic of my talk was Machine Learning (ML) for Business Processes (BP). There are many reasons for choosing this topic: first, ML has reached that stage where models are being deployed in real world, but they still face problems related to labelled data which is expensive to get’; second, this is what I worked on during my final semester internship at RGU and am working in my PhD. . About the Talk . In this talk, I motivated the students to focus on efficient ways of gathering data in ML ecosystem before going into the learning algorithms. I also gave an example on how to use one such method, Active Learning (AL), for a classification problem. For a quick intro, AL is a method where learning algorithms are allowed to choose the data from which they learn, hence achieving greater accuracy with fewer training labels available. . In this digital-era, although the demand for Data Scientists is high, we can’t ignore the fact that supply is also not low. Thus, I wanted to draw students’ attention towards working on real-world ML ecosystem where these algorithms are required to learn better in a low resource setting as well. That’s why, having some experience in Low Resoucre ML is always going to set them aside from the crowd. . Acknowledgement . First of all, I would like to thank the whole IIITNR family for giving me this wonderful opportunity. I would especially like to thank Vice-Chancellor &amp; Director, Dr PK Sinha for his constant motivation and encouragement. He has always tried to get the best out of his students. I would like to thank the Alumni Association of IIITNR, especially Dr Santosh Kumar and Dr PP Paltani for organising such a wonderful event, that too in online mode. I would also like to thank Dr Muneendra Ojha for always guiding me and motivating me to present my work in this event. . I wish the students of IIITNR very best of luck in their journey. Here are the slides from my talk: .",
            "url": "https://ashishu007.github.io/nlp-blog/blog/machine%20learning/active%20learning/labelled%20data/business%20processes/2020/08/28/ML-for-BP.html",
            "relUrl": "/blog/machine%20learning/active%20learning/labelled%20data/business%20processes/2020/08/28/ML-for-BP.html",
            "date": " • Aug 28, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Deep Learning in Natural Language Processing",
            "content": "Deep Learning in Natural Language Processing . I read many papers about Deep Learning (DL) in Natural Language Processing (NLP) in the starting months of my PhD. This blog is a summary of that reading, where I try to sequentially discuss the development of DL methods applied to various NLP tasks. . A little heads-up about the blog. This blog is not an in-depth analysis of DL methods applied to NLP tasks, instead, I have tried to provide a quick overview of the main development of DL methods for NLP. The goal of this post is to provide the reader with a direction for starting with DL in NLP. I’ll try to showcase the field’s evolution in a previous couple of years. Hopefully, by the end of this blog, if you are a newcomer to the field, you’ll at least have the understanding of technical terms of DL in NLP. . So, without further ado, let’s start. &lt;!– Table of Contents . Artificial Intelligence - Looking Back | Word Embeddings | Sequence Learning Vanilla seq2seq Model | seq2seq with Attention | . | Learning Methods Recurrent Neural Networks | Long Short-Term Memory Networks | Convolutional Neural Networks | Transformers | . | Language Models | Transfer Learning Embeddings Learned from Language Models | Universal Language Model Fine-Tuning | Generative Pre-Training | Bidirectional Encoder Representation from Transformers | . | Further Steps | Acknowledgement . | References | . generated with markdown-toc –&gt; . Artificial Intelligence - Looking Back . First, a quick recap of the developments in Artificial Intelligence (AI) research. In the early days of AI, before the 90s, experts used to design task specific rules for different real-world problems. However, they would often fail when unseen or unexpected data/situation arrived. . In the last 20 years, around the late 90s or so, statistical approaches to these problems started gaining traction. Now, instead of writing rules, human effort was involved in extracting different features that would tell a mathematical model to learn the relation between input and output space. Statistical models are capable of learning the rules themselves, by looking at the labelled examples. These approaches can be considered as the early days of machine learning. However, they still require domain expertise to engineer domain specific features. . Around 2010, when a huge amount of data and powerful computers started coming out, a subset of machine learning, called deep learning, using neural networks became a model of choice for learning from data. These models are capable of learning the multi-level hierarchy of features and thus do not require any explicit need for feature engineering. Human energy is now focused on determining the most suitable architecture and training setting for each task. . . Sesame Street &amp; Co.: Popular naming convention nowadays in NLP (source) From next sections, I’ll talk about NLP. . Word Embeddings . The first thing to start should be the representation of textual data in numerical format. Word Embeddings are used to represent words in a multi-dimensional vector form. A word wi in vocabulary V is represented in the form of a vector of n dimensions. These vectors are generated by unsupervised training on a large corpus of words to gain the semantic similarities between the words. Algorithms like word2vec [1] and GloVe [2] are used to train these word embeddings. These word embeddings are generally pre-trained and made publicly available to be directly used in the deep learning models. . . 2-D visualisation of GloVe vectors (source) These distributed representations of words give a certain amount of semantic understanding of the words in a high dimensional vector space. For example, the distance between words king and queen will be similar to the distance between boy and girl. In this contrast, the below equation fits perfectly. . boy - girl + king = queen These representations have a drawback that they fail to take the context of a word into account. For example, the word ‘Scotland’ will have a different meaning in the sentence ‘Scotland is one of the best places to live on earth’ than in the sentence ‘Royal Bank of Scotland is one of the top banking firms in the UK’. The GloVe or word2vec word embeddings will fail to differentiate the two meanings of Scotland and will assign the same vector in both the cases. These drawbacks are tackled by a new concept of contextual word embeddings discussed in a section below. . Other Resources This lecture from CS224u. | Word2Vec Paper. | GloVe Paper. | . | . Sequence Learning . Most NLP tasks require sequential output instead of a single output label, unlike classification or regression, for example, Machine Translation, Question-Answering, or Named-Entity Recognition. These systems take a sequence of input and process it to produce yet another sequence for output. The goal is to take a sequence x1, x2, …, xn as input and map it to another sequence y1, y2, …, ym as output. . Vanilla seq2seq Model . The architecture used to deal with this kind of problems is known as Sequence-to-Sequence model or in common terms, seq2seq model. It is a combination of encoders and decoders which works in a sequential manner, where, an encoder is a neural network that generates a context vector from the input sequence, and decoder, another neural network taking context vector as input, generates the output sequence. . The encoder takes an input X and maps it to fixed-size context vector Z using the formula given below: . Z = &sigma;(Wx + b) where σ is an activation function. A decoder then maps the context vector Z to a new form of input X’ as shown in the equation below: . X&#39; = &sigma;&#39;(W&#39;Z + b&#39;) where σ’ is another activation function. The loss is calculated as the squared error between original and reconstructed input as shown in equation below: . L = || X - X&#39; ||2 An illustrated diagram of seq2seq model performing machine translation is shown in the figure below: . . Encoder . Decoder An Encoder-Decoder seq2seq model (source) seq2seq with Attention . A problem with general encoder-decoder seq2seq model is that they give equal importance to all parts of the input sequence. Also, the input sequence is compressed into a single context vector which creates the bottleneck problem, where a piece of long information is tried to be compressed into one small representation. . A solution to this problem was proposed in the work [3] introducing a new mechanism called Attention. Attention aligns the output at each decoding step to the whole input sequence in order to learn the most important part of the input aligning with the current step output. . Let’s say we have calculated the encoding hidden states h1, h2, …, hn for the input sequence x1, x2, …, xn during the calculation of context vector Z. For a decoder hidden state st on timestep t, we get attention score et as follows: . We take the softmax of these scores to get the attention distribution at timestep t. . The attention output at is then calculated as the weighted sum of encoder hidden state using ∝t: . Finally, we concatenate the attention output at with decoder hidden state st and proceed to calculate the negative log loss same as the non-attention decoder model. . An attentive machine translation model is shown in the figure below: . . Attention model (source) Other Resources Have a look at this lecture from CS224n. | Read the seq2seq paper. | Read this attention paper. | . | . Learning Methods . Now let’s talk about the learning algorithms that are capable of learning the mapping between input and output representations. Here, we’ll take a look at some of the backbone neural networks (DL architectures) used in various NLP tasks. . Recurrent Neural Networks . Text is a sequential form of data. To process text and extract information from it, we need a model capable of processing the sequential data. Recurrent Neural Networks (RNNs) [4] are the most elementary deep learning architectures that are able to learn from sequential data. RNNs are wide in nature as they unroll through time. These networks will have a ‘memory’ component which can store the information about the previous state. They share the same set of weights throughout the layers, however, will receive a new input at every layer or time-step. . The output to every time-step is dependent on the input taken at the current time-step ti as well as the information gained from previous time-step ti-1. Specifically, an RNN will maintain a hidden state ht at every step which is referred as the memory of network. An illustrated diagram of unrolled RNN is shown in figure below . . A simple Recurrent Neural Network (source) The operations performed in RNN at every time step is given in the equations below: . ht = &sigma;h ( Wext + Whht-1 + bh ) yt = &sigma;y ( Wyht + by ) Here σy and σh are the activation functions. Wh is the weight matrix to apply transformation on previous hidden state ht-1, We is the weight matrix to apply transformation on the input xt received over time t. Combining these with the bias bh yields hidden state ht for time t. Applying activation on the ht with Wy gives the output yt for every time-step t. . Other Resources Have a look at this lecture from CS224n. | . | . Long Short-Term Memory Networks . Although, in theory the RNNs are designed to handle the sequence input but in practice they lack in storing the long term dependencies because of the problem of exploding and vanishing gradients [5]. Long Short-Term Memory Networks [6] are the advanced version of RNNs with a slight modification of being capable of deciding what to ‘remember’ and what to forget from the input sequence with the help of a series of gates. LSTMs have a number of gates: an output gate ot; an input gate it; a forget gate ft - all of which are the functions of previous hidden state ht and current input xt. These gates interact with the previous cell state ct-1, the current input xt, and the current cell state ct and enable the model to selectively retain or information from the sequence. The full version of LSTM is given in the equation below. . ft = &sigma;g ( Wfxt + Ufht-1 + bf ) it = &sigma;g ( Wixt + Uiht-1 + bi ) ot = &sigma;g ( Woxt + Uoht-1 + bo ) &#265;t = &sigma;c ( Wcxt + Ucht-1 + bc ) ct = ft &amp;#x25CB; ct-1 + it &amp;#x25CB; &#265;t ht = ot &amp;#x25CB; &sigma;h(ct) where σg is the sigmoid activation function, σc and σh are the tanh activation function, and ○ is element-wise multiplication, also known as Hadamard product. An illustrated diagram of LSTM is shown below: . . A simple Long-Short Term Memory Network (source) LSTM layers can be stacked on each other to form multi-layer LSTM architecture. One of the most popular LSTM architecture is Bidirectional LSTM (BiLSTM), where two separate LSTMs are used to capture the sequential information in both forward and backwards directions. Gated Recurrent Units (GRUs) [7] are another advanced version of RNN which are common for handling the vanishing gradient problem, but I am not discussing them here as they are almost similar to LSTM except with no cell-state. . Other Resources Go through this amazing blog from Christopher Olah. | . | . Convolutional Neural Networks . Convolutional Neural Networks (CNNs) [8] have established themselves as state-of-the-art in various computer vision tasks. For the last couple of years, CNNs have been the benchmark for almost every vision task. Inspired from the popularity of CNN in vision, Yoon Kim proposed a CNN architecture for sentence classification which outperformed then benchmarks on various text classification datasets [9]. . A CNN takes an input sentence of n words, where each word is represented using a vector of dimension d. The input X1:n will be a 2D matrix of shape n × d, where xi ∈ ℝ d . Input X1:n can be represented as: . X1:n = x1 &oplus;, ..., &oplus; xn where ⊕ is the concatenation operator. On the input layer, convolution filter W ∈ ℝhd is applied over window of h words to generate a new feature. So, a feature ci is generated from the word window xi:i+h-1 with the following operation: . xi = &sigma;(Wxi:i+h-1 + b) where W is the weight matrix for the connections, σ is the activation function and b ∈ ℝ is the bias. Now, this filter is applied to each possible window of words giving an feature map C ∈ ℝn-h+1. . C = [c1, c2, ..., cn-h+1] The entries in feature map C are sharing the parameter W, where each ci ∈ C is a result of calculation on small segment of the input. Then a max-pooling operation is applied on these feature maps to capture the most important part. . &#264; = max(C) This parameter sharing helps the model to incorporate an inductive bias into the model, helping to become learn the location invariant local features. There are k number of filters applied to the input with different window sizes which are then concatenated to form a vector K ∈ ℝk. Which is then fed to the next hidden layer or output layer. . An illustrated diagram of a CNN architecture for text classification is shown in the figure below: . . A simple CNN for Text Classification (source) Other Resources Have a look at the CNN for text classification paper. | Also have a look at this lecture from CS224n. | . | . Transformers . Most of the above models have recurrent behaviour, which can not be trained parallelly. This imposes a huge problem of time taken to train a model from scratch. In the work [10], authors proposed a new neural architecture called Transformers which uses a combination of self-attention and feed-forward network and doesn’t require any recurrent or convolutional elements. Also, the self-attention module lets the model capture the long term dependencies in a sentence without having any effect of the sentence length. . This new model was a huge success gaining better performance on various sequential learning tasks. To name one, it improved the machine translation performance by 10 BLEU on WMT EN-FR and WMT EN-DE datasets. It also reduced the training time by large margin benefiting from the non-recurrent behaviour. . The success of transformer architecture paved the way for the development of new models to solve the sequential tasks. It helped NLP researchers to utilize its non-recurrent nature in transfer learning where the transformer is used for general pre-training of a language model (LM). Then the LM fine-tuned on domain-specific dataset for downstream tasks. An encoder-decoder model using Transformer architecture is shown in the figure below: . . A Transformer (source) I cannot provide an in-depth overview of the Transformer but you can refer to resources below for a better understanding. . Other Resources Go through the main paper “Attention is all you need”. | Take a look at this amazing blog. | Must read - Jay Alammar’s blog on Transformers. | . | . Language Models . Another important concept we should be aware of in NLP is Language Models (LM), because transfer learning in NLP is applied using different versions of LMs only. A language model is a type of system that predicts the probability of possible next words for a given sequence of words as the input. . . A simple Language Model (source) In general terms, for a given sequence of input (x1, x2, …, xt) the probability distribution of next term (xt+1) is computed from a vocabulary V of k words (V = (w1, w1, …, wk)) as given below: . P(xt+1|x1, ..., xt) Earlier the language models were based on statistical approaches where they used to take a window of n words as a context from the sentence to predict the next word. This approach is also known as n-gram Language Model. It takes a simple approach of calculating the conditional probability of next word in the sentence given the window of n words as a context. These n-gram probabilities are calculated from counting them in some large corpus of text. . P(xt+1|x1, ..., xt) = P(xt+1, xt, ..., xt-n+2) &frasl; P(xt, ..., xt-n+2) or in simpler terms: . P(w3|w1, w2) = P(w1, w2, w3) &frasl; P(w1, w2) This statistical approach has mainly two problems. First, sparsity - consider the above equation, what if w1, w2, w3 never occurred together, the probability of w3 will be 0. Second, storage - as we increase the value of n, the count of all n-grams we see in the corpus increases as well and so does the need of memory to store them. . The neural language models using RNNs and Transformers are capable of modelling all the words in a sentence without needing a window to predict the next word at each timestep. You can think of a simple RNN for neural LM, where the RNN takes input tokens one at a time and by processing them generates the hidden state of each timestep. At the final time step, the output of that RNN block will be taken as the softmax against the whole vocabulary, and the word with the highest probability will be the prediction of LM as next word in the sequence. . Other Resources For a better and deeper understanding please refer to this lecture from CS224n. | . | . Transfer Learning . NLP cracked transfer learning by applying a simple rule: first, learn the general nuances of text (grammar-rules or fill in the blanks) from a huge corpus of text using a language model; and then transfer that learning by fine-tuning the language model on a task-specific dataset. In the following sections, we’ll discuss some of the advancements in using different versions of LMs for transfer learning. . Embeddings Learned from Language Models . As discussed in the above section, the word embeddings generated by algorithms like word2vec and GloVe lack the contextual awareness and fail to differentiate a word with different senses. A new version of words embeddings, known as, contextual word embeddings are capable of differentiating a word’s sense based on its context. These new models use a Language Model (LM) to generate the contextualised representation of words in a sentence. These modified embeddings generated from the LMs can be used as the input to another neural network for some downstream tasks. . ELMo [11] is one such embedding algorithm that uses a bidirectional language model to capture the context of a word in a sentence from both sides (left to right and vice-versa). ELMo uses a character-level CNN to convert raw text into a word vector which is then fed into a bidirectional language model. The output of this BiLM is then sent to the next layer of BiLM to form a set of intermediate word vectors. The final output of ELMo is the weighted sum of raw vectors and the intermediate vectors formed from two layers of the BiLMs. The two language models used here are based on LSTM architectures. An illustration of ELMo is shown in the figure below: . . ELMo Model (source) ELMo achieved 9% error reduction on the SQuAD (question-answering) dataset compared to then SOTA, 16% on Ontonotes SRL dataset, 10% on Ontonotes coreference dataset and 4% on CoNLL 2003 dataset. The fact that these embeddings are contextual and have the knowledge of word senses, helps ELMo and other future models such as BERT and GPT perform so much better. . Other Resources Before ELMo, CoVe proposed a similar idea. Have a look at the paper, if you fancy reading. | Read the ELMo Paper. | Recommended notes for a quick read. | Have a look at this lecture from CS224u. | Also, go through this amazing blog on Analytics Vidhya. | . | . Universal Language Model Fine-Tuning . Universal Language Model Fine-Tuning (ULMFiT) [12] can be considered as one of the pioneers of applying transfer learning on an NLP task (text classification). It does so in three main steps: first, training a general domain-independent language model on a large corpus of text; second, fine-tune the language model on task-specific target dataset; and third, again fine-tuning the fine-tuned language model as a classifier by adding a softmax activation on top with target dataset. An illustration of the three steps of ULMFiT is shown in the figure below: . . ULMFiT Model (source) ULMFiT achieved better results for text classification on six different datasets ranging from topic classification to sentiment analysis. The fine-tuning approach employed by ULMFiT is also very interesting, where different learning rates are applied to different layers of the network. I would recommend reading the paper to get a better understanding of the updated version of backpropagation through time algorithm. . Other Resources Definitely read the ULMFiT Paper. | Have a look at this amazing blog. | . | . Generative Pre-Training . One of the earliest works in using Transformers for pre-training of language model and applying transfer learning was presented in Generative Pre-Training (GPT) [13]. Following the idea from ELMo, authors proposed a language model using transformer decoder trained on a large corpus of text. The main difference of GPT from ELMo is that ELMo uses two independent LSTM language models to capture the forwards and backward context whereas, in case of GPT, it uses a uni-directional multi-layer transformer language model capable of capturing context due to its attentive nature. . ELMo takes a feature-based approach of generating feature vectors (or contextual representations of the sentences) for different tasks, whereas GPT takes a fine-tuning based approach where the same language model trained on huge corpus is fine-tuned on task-specific data for downstream tasks. An illustration of a GPT model used for pre-training is shown in the figure below: . . GPT Model (source) Other Resources Read the GPT Paper. | The updated version GPT-2 Paper. | A blog post similar to this one. | . | . Bidirectional Encoder Representation from Transformers . Bidirectional Encoder Representation from Transformers (BERT) [14] is another example of the success of transfer learning in NLP. BERT is a bidirectional transformer language model trained on a large text corpus that can be fine-tuned on any domain-specific dataset for the downstream tasks such as text classification, or named entity recognition. BERT mainly differs from other models like GPT and ELMo because of the pre-training tasks used during the unsupervised training of language model. Its pre-training is based on two tasks: first, Masked Language Model (MLM); and second, prediction of next sentence from the corpora. . For the first task of Masked Language Model, let’s say we have a sentence ‘Boris Johnson is the Prime Minister of UK’. So instead of training for prediction of next word in the sentence as a general Language Model, BERT pre-training replaces 15% of the words with a [MASK] token and learns to predict the correct word at the position of [MASK] token. In the second task of Next Sentence Prediction, the model is trained to learn the relationship between sentences where, for a given sentence pair A &amp; B, the model is asked to predict if the sentence B is the next sentence that comes after A. . BERT improved the fine-tuning based approach of GPT by using a bidirectional transformer and learning both left &amp; right context at the same time. This gave a huge improvement over GPT’s unidirectional approach especially for token-level tasks like Question Answering, where the answer depends on both left and right contexts. An illustrated diagram of BERT pre-training and fine-tuning is shown in the figure below: . . BERT Model (source) A visual comparison between BERT, GPT and ELMo architectures presented in the paper is shown below: . . BERT, GPT and ELMo (source) We can see, BERT uses the bi-directional transformer for processing the sequence, while GPT uses a uni-directional transformer. On the other hand, ELMo uses bi-directional LSTM to capture the bi-directional context while processing the input. . Other Resources Read the BERT Paper. | Have a look at this amazing blog by Jay Alammar). | . | . Further Steps . If you have made this far, then why not have some awesome resources for your further exploration. . I would recommend looking at these amazing repositories: NLP-Progress: An open-source repo that tracks the SOTA in several NLP tasks in multiple languages. | Awesome NLP: Another open-source repo providing a single place for many NLP resources. | . | If you would like to study from some online courses, here are my top recommendations: fast.ai Course: A Code-First Introduction to NLP | Stanford’s CS224n: Natural Language Processing with Deep Learning | Stanford’s CS224U: Natural Language Understanding | . | . Acknowledgement . The blog’s idea is highly inspired by Sebestian Ruder’s PhD thesis. Especially the background chapter, where a quick overview of deep learning is provided before proceeding to the contributions of the thesis. . | I would like to thank Dr Muneendra Ojha and Krutika Bapat for reviewing the blog. . | I took the help of numerous online resources to get a better understanding of this field. Some are listed here if I have missed any blog/resource - sincere apologies for that. Please comment here or write an email, I’ll properly acknowledge them. . Sebestian Ruder’s Blogs | Christopher Olah’s Blogs | Jay Alammar’s Blogs | Stanford’s CS224n Course | Stanford’s CS224u Course | Analytics Vidhya Blog | . | . References . [1] Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S. &amp; Dean, J. (2013), Distributed representations of words and phrases and their compositionality,in‘Advances in neural information processing systems’, pp.3111–3119. . [2] Pennington,J., Socher,R. &amp; Manning,C. (2014), Glove: Global vectors for word representation, in ‘Proceed-ings of the 2014 conference on empirical methods in natural language processing (EMNLP)’, pp. 1532–1543. . [3] Bahdanau, D., Cho, K. &amp; Bengio, Y. (2014), ‘Neural machine translation by jointly learning to align and translate’,arXiv preprint arXiv:1409.0473. . [4] Elman, J. L. (1990), ‘Finding structure in time’,Cognitive science14(2), 179–211.Graves, A., Jaitly, N. &amp; Mohamed, A.-r. (2013), Hybrid speech recognition with deep bidirectional lstm, in ‘2013 IEEE workshop on automatic speech recognition and understanding’, IEEE, pp. 273–278. . [5] Bengio, Y., Simard, P., Frasconi, P. et al. (1994), ‘Learning long-term dependencies with gradient descent is difficult’, IEEE transactions on neural networks. . [6] Hochreiter, S. &amp; Schmidhuber, J. (1997), ‘Long short-term memory’, Neural computation 9(8), 1735–1780. . [7] Cho, Kyunghyun, et al. “Learning phrase representations using RNN encoder-decoder for statistical machine translation.” arXiv preprint arXiv:1406.1078 (2014). . [8] LeCun, Y., Bottou, L., Bengio, Y., Haffner, P. et al. (1998), ‘Gradient-based learning applied to document recognition’,Proceedings of the IEEE86(11), 2278–2324. . [9] Kim, Yoon. “Convolutional Neural Networks for Sentence Classification.” Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2014. . [10] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł. &amp; Polosukhin, I.(2017),Attention is all you need, in‘Advances in neural information processing systems’, pp.5998–6008 . [11] Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K. &amp; Zettlemoyer, L. (2018), ‘Deep contextualized word representations’,arXiv preprint arXiv:1802.05365. . [12] Howard, J.&amp;Ruder, S.(2018), ‘Universal language model fine-tuning for text classification’, arXiv preprintarXiv:1801.06146. . [13] Radford, A., Narasimhan, K., Salimans, T. &amp; Sutskever, I. (2018), ‘Improving language un-derstanding by generative pre-training’. . [14] Devlin, J., Chang, M.-W., Lee, K. &amp; Toutanova, K. (2018), ‘Bert: Pre-training of deep bidirectional transformers for language understanding’, arXiv preprint arXiv:1810.04805. .",
            "url": "https://ashishu007.github.io/nlp-blog/blog/nlp/transfer%20learning/deep%20learning/transformers/lstms/rnns/2020/07/21/DL-in-NLP.html",
            "relUrl": "/blog/nlp/transfer%20learning/deep%20learning/transformers/lstms/rnns/2020/07/21/DL-in-NLP.html",
            "date": " • Jul 21, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Introduction to Data-to-Text Generation",
            "content": "Introduction to Data-to-Text Generation . After wandering in the vast NLP research field for some time, I finally decided to work towards data-to-text generation in my PhD. In this blog, I’ll try to provide: a brief overview of the task’s requirements; some standard public datasets available; and the evaluation metrics used for measuring the performance on these datasets. . Natural Language Generation . First, I’ll start with a small introduction on Natural Language Generation (NLG). In NLG, the requirement is to generate a textual document (in some natural language) for the given input (in some format). . There are several real-world applications to automated text generation. Here are some examples. . . | | . Take the Medical Reporting domain for instance - suppose there’s a doctor who has to analyse the data from some patients different medical test results. The doctor will have to analyse each test’s result in order to make a decision on the patient’s condition. A summary of these test results in a textual format highlighting the main parts can be very benificial to the doctor and will reduce a lot of their time and effort required. . Take another example of Weather Forecasting - a textual report about the weather conditions summarising the huge numerical data can be very benificial for a meteorologist. Even for the general public, those reports can be very helpful in providing the weather information breifly. . Based on the input provided to the system, NLG can be broadly categorised into two different categories: first, text-to-text generation (T2T NLG); and second, data-to-text generation (D2T NLG). . T2T NLG . As the name suggests, in text-to-text generation, our goal is to generate text from unstructured textual input. For example, machine translation, where we take a text document in one natural language as input and produce the same content in different natural language as output. . . Text-to-Text Natural Language Generation (T2T NLG) . D2T NLG . For data-to-text generation, the input is presented in a structured format, i.e., tablular, graphical or JSON format. With this structured input, we generate a textual output summarising the input values. For example, summarising NBA match where, for given box- and line-scores as input we have to generate a textual summary of the match as the output. . . Data-to-Text Natural Language Generation (D2T NLG) . In general, automated text generation is challenging because grammar rules are very complex, and also, there can be several meaning to same words in different context. . Even after we develop an automated system for text generation, it is challenging to automatically evaluate the texts generated from that system. Unlike most supervised problems, there’s no class knowledge in form of labels to evaluate the performance. Here the system’s goal is to generate accurate, fluent; and diverse texts, not just predicting some label like in most of the other NLP tasks. . Subtasks in D2T NLG . Data-to-Text generation is a long process. It invovles a lot of things - selecting important insights from the data to finally generating the textual document summarising that data. In general, the whole pipeline of D2T can be broadly categorised into these different subtasks: . Content Determination: deciding which information from the input will be included in the final text; . | Text Structuring: select the ordering of the selected information in the final text output; . | Sentence Aggregation: selecting which information to be presented in a separate sentence and which two (or more) information can be presented in the same sentence; . | Lexicalisation: finding the correct words and phrases to express the information in a sentence; . | Referring Expression Generation: selecting domain-specific words and phrases; and . | Realisation: Combining all the words and phrases into well-formed sentences. . | . Let’s discuss these subtasks with an example. The figure below illustrates a simplified example from the neonatal intensive care domain. . . . The tasks that needs to be performed in order to generate the full textual summary from the input data can be described as follows: first the system has to decide what the important events are in the data (a, content determination), in this case, occurrences of low heart rate (bradycardias); then it has to decide in which order it wants to present data to the reader (b, text structuring) and how to express these in individual sentence plans (c, aggregation; lexicalisation; reference); finally, the resulting sentences are generated (d, linguistic realisation). | . Subtasks in D2T NLG (source) . The specification of these subtasks vary from domain to domain, but the basic idea remains the same. . Let’s take another example, the NBA match summarisation (the D2T NLG figure shown above) for instance. Here also, the generation will happen in the following steps: . first, we need to decide what records from the input table will be dispalyed in the final text (or what to say?); | second, we’ll have to decide in what order those records will be displayed, which will also include the deciding on which records will have separate senetences and which ones will be included in the same sentence (or how to say?); and | finally, generating the text by combining all the decisions made in previous steps (or saying what’s decided). | . Public Datasets and Evaluation Metrics . Now that we know about the expectations in D2T NLG, let’s see some of the standard datasets available in public domain and evaluation metrics used to measure the performance of different methods on these datasets. . To keep track of the state-of-the-art in this field, I would recommend to follow this article on NLP-progress or this task category on Papers with Code. . RotoWire . The dataset consists of articles summarizing NBA basketball games, paired with their corresponding box- and line-score tables. It is professionally written, medium length game summaries targeted at fantasy basketball fans. The writing is colloquial, but structured, and targets an audience primarily interested in game statistics. The picture used above for the example of D2T NLG is from this dataset only. . The performance is evaluated on two different automated metrics: first, BLEU score; and second, a family of Extractive Evaluations (EE). EE contains three different submetrics evaluating three different aspects of the generation. Since EE metrics are comparatively new than others, I’ll briefly explain them here: . Content Selection (CS): precision (P%) and recall (R%) of unique relations extracted from generated text that are also extracted from golden text. This measures how well the generated document matches the gold document in terms of selecting which records to generate. . | Relation Generation (RG): precision (P%) and number of unique relations (#) extracted from generated text that also appear in structured input provided. This measures how well the system is able to generate text containing factual (i.e., correct) records. . | Content Ordering (CO): normalized Damerau-Levenshtein Distance (DLD%) between the sequences of records extracted from golden text and that extracted from generated text. This measures how well the system orders the records it chooses to discuss. . | . I am not explaining other evaluation metrics here, I’ll try to do that in some later post. . WebNLG . The WebNLG challenge consists in mapping data to text. The training data consists of Data/Text pairs where the data is a set of triples extracted from DBpedia and the text is a verbalisation of these triples. For example, given the three DBpedia triples (as shown in [a]), the aim is to generate a text (as shown in [b]): . [a]. (John_E_Blaha birthDate 1942_08_26) (John_E_Blaha birthPlace San_Antonio) (John_E_Blaha occupation Fighter_pilot) . | [b]. John E Blaha, born in San Antonio on 1942-08-26, worked as a fighter pilot. . | . The performance is evaluated on the basis of BLEU, METEOR and TER scores. The data from WebNLG Challenge 2017 can be downloaded here. . Meaning Representations . The dataset was first provided for the E2E Challenge in 2017. It is a crowd-sourced data set of 50k instances in the restaurant domain.Each instance consist of a dialogue act-based meaning representations (MR) and up to 5 references in natural language (NL). For example: . MR: name[The Eagle], eatType[coffee shop], food[French], priceRange[moderate], customerRating[3/5], area[riverside], kidsFriendly[yes], near[Burger King] . | NL: “The three star coffee shop, The Eagle, gives families a mid-priced dining experience featuring a variety of wines and cheeses. Find The Eagle near Burger King.” . | . The performance is evaluated using BLEU, NIST, METEOR, ROUGE-L, CIDEr scores. The data from E2E Challenge 2017 can be downloaded here. . Further Steps . For a detailed review of the field, I would recommed reading this survey paper. | Here you can find a list of public datasets available for D2T NLG. | Have a look at the ACL’s Special Interest Group on Natural Language Generation - ACL SIGGEN. | Would recommend to follow Prof. Ehud Reiter’s Blog, he writes a lot on the issues of NLG (alsow w/ ML/DL) research. | .",
            "url": "https://ashishu007.github.io/nlp-blog/nlp/d2t/nlg/2020/06/22/Data-to-Text-Generation.html",
            "relUrl": "/nlp/d2t/nlg/2020/06/22/Data-to-Text-Generation.html",
            "date": " • Jun 22, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "To know more about me, check out my website. .",
          "url": "https://ashishu007.github.io/nlp-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ashishu007.github.io/nlp-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}