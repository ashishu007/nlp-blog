{
  
    
        "post0": {
            "title": "Installing Elasticsearch and Kibana",
            "content": "Install Elasticsearch and Kibana on Windows . Since both ElasticSearch and Kibana is written in Java, its mandatory to have the current JDK installed. If not, go to this page and install the latest JDK. . Download and Unzip Packages . Download the Windows .zip packages for elasticsearch and kibana from the following links: Elasticseacrh: https://www.elastic.co/downloads/elasticsearch | Kibana: https://www.elastic.co/downloads/kibana | . | After downloading, unzip the files to some location. Let’s say Elasticseacrh: C: Users ashis Downloads elasticsearch-7.9.2-windows-x86_64 | Kibana: C: Users ashis Downloads kibana-7.9.2-windows-x86_64 | . | . Verify the JAVA_HOME path . Since both tools need JDK to run, we need to verify if the JAVA_HOME path is set correctly. For this follow these steps: . Under the search bar type env and click the Edit the System Environment Variables. | . Then click the Environment Variables option. | . Now, check if the value for JAVA_HOME is set to something like this: C: Program Files java jdk*. The one highlighted in blue is the path for JAVA_HOME: | . If JAVA_HOME isn’t setup, follow these setps: Copy the path for your JDK, it should be something like: C: Program Files Java jdk1.8.0_261 (depending on the version of Java installed in your system) | In the Environment Variables window, click New for System Variables | In the field Variable name write JAVA_HOME | In the field Variable value paste the JDK path that you’ve copied | . | . Running Elasticsearch . Open the command-prompt (type cmd in the search bar and click enter) and navigate to the folder where you unzipped elasticsearch. . | Run bin elasticsearch.bat . | Elasticsearch will start on that cmd. . Note: If you wish to explicitly define the cluster and node names, please use the following command: . bin elasticsearch.bat -Ecluster.name=my_cluster -Enode.name=my_node . When the cmd turns to this: | . Open http://localhost:9200/ in the browser. You should see something like this: | . Running Kibana . Open another command-prompt and navigate to the folder where you unzipped kibana. . | Run bin kibana.bat . | When cmd turns to this: . | . Open http://localhost:5601/ in the browser. You should see something like this: | . Experiments . Now everything is installed, goto this page for experimentation. .",
            "url": "https://ashishu007.github.io/nlp-blog/markdown/ir/elasticsearch/kibana/2020/11/23/install-es-kb.html",
            "relUrl": "/markdown/ir/elasticsearch/kibana/2020/11/23/install-es-kb.html",
            "date": " • Nov 23, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Elasticsearch and Kibana for Information Retrieval",
            "content": "Elasticsearch and Kibana for Information Retrieval . Objectives . The objectives of this lab are: . to explore the basic functionality of a state-of-the-art IR system; and | to compare the retrieval effectiveness of BM25. | . Approach . You will install and run Elasticsearch and employ a console available on Kibana to load and search our document collections (Documents and IRDocuments). . To install Elasticsearch and Kibana, goto this page. . Elasticsearch . In the previous labs we have used a simple stand-alone retrieval engine. This has been great to allow us to easily try different approaches to indexing. However, it is not suitable for real production applications because it is not optimised to scale for large volume data in a cloud computing, client-server architecture. There are 2 widely used, open-source IR systems that can scale to massive volume applications. These are Apache Solr (https://lucene.apache.org/solr/) and the more recent ElasticSearch (https://www.elastic.co). Both are built in Java, based on an IR engine called Lucene, and provide a server-side IR system that can be accessed via a REST API. ElasticSearch has become the industry standard for new applications. . ElasticSearch (ES) is a distributed, open source search and analytics engine for all types of data, including textual, numerical, geospatial, structured, and unstructured. Its speed and scalability along with the ability to index many types of content mean that it can be used for a wide range of applications. Core to its performance is an inverted index very similar to those we have already seen. . Kibana is a data visualization and management tool for Elasticsearch that provides real-time histograms, line graphs, pie charts, and maps. It also contains a simple console that we will use to interact with the ES API using JSON data format. . Install and Run ElasticSearch and Kibana . If you navigate to the Maths + Statistics folder in the AllPrograms section of your startup menu, you should find ElasticSearch and Kibana already downloaded and installed. . | Click first on the ElasticSearch folder to run the .bat file. You should see a command line interface open up as ElasticSearch starts up and runs in the background on a local server on your desktop. . | Go to http://localhost:9200/ and you should see something like this: . | . Now select the Kibana folder to run the .bat file. It should now open in the command line window. Point your browser at http://localhost:5601 and you should see something like this: | . On the top-left corner, click the three parallel lines (≡) to open the menu. Now scroll-down to find the Dev Tools option under Management section. Click it to go to the console where we will be writing the queries. . In a production system Elasticsearch would typically run on a cloud server with client side JavaScript applications passing information back and fore in JSON data format to the API made available by Elasticsearch. The Kibana console allows us to simulate sending requests (in the left-hand pane) and receiving answers (in the right-hand pane). This allows us to explore Elasticsearch without building the frontend application . ElasticSearch is open source and can be installed on your own machine. Check out this page for information on installing on Windows. . Add Individual Documents to the Index . First ensure that an Elasticsearch instance is running on http://localhost:9200 and a Kibana instance is running on http://localhost:5601. . As we have a clean Elasticsearch instance initialized and running, the first thing we are going to do is to add documents to an index and then retrieve them. Documents in Elasticsearch are represented in JSON format. . We are adding now to the index named irdocuments (the index name should be in lowercase only) a document having the id 1; since the index does not exist yet, Elasticsearch will automatically create it. . Run the following query in the Kibana console by clicking the green play-button: . POST irdocuments/_doc/1 { &quot;title&quot;: &quot;Document 1&quot;, &quot;content&quot;: &quot;The cat sat on the mat next to the cat.&quot; } . The response on the right should be something like this (if you copy and paste watch out for quotation mark errors): . . Verify that the document has been added trying: . GET irdocuments/_doc/1 . The response should be like this: . . Add the other 3 documents from IRDocuments and verify they have been added. You can try the following query to retrieve all the documents from the index irdocuments: . GET irdocuments/_search { &quot;query&quot;: { &quot;match_all&quot;: {} } } . Try: . GET irdocuments/_count . to count documents in the index. . Let’s delete document 1 and index a new doc with the text “The white cat sat on the mat next to the black cat.” Verify the result. . To delete a document from the index, run this query: . DELETE irdocuments/_doc/1 . Now you can delete the whole index by running: . DELETE irdocuments . Verify you have been successful. . Indexing a Set of Documents in Elasticsearch . Elasticsearch offers a Bulk API that allows us to perform add, delete, update and create operations in bulk for many documents at a time. Since the documents in Elasticsearch are represented in JSON format, we first need to convert our txt files into a predefined JSON format. This has already been done for us for our larger dataset with a python script (see Appendix-Py-script), the output of which is available as all_docs.json in lab resources on Moodle. . To get more insight for Bulk API please visit this link. . Let’s get this data into Elasticsearch. Since the body of these requests can be big, it is recommended to do this via a tool that allows to load the body of a request from a file - for instance, using curl. Open a command prompt and navigate to the folder where all_docs.json file is saved. Run the following command: . curl -H &quot;Content-Type: application/x-ndjson&quot; -XPOST &quot;localhost:9200/documents/_bulk?pretty&quot; --data-binary @all_docs.json . This command will index all these documents to an index named documents. Be patient, it can take a while to upload the json. Verify the indexing by running the following query: . GET irdocuments/_search { &quot;query&quot;: { &quot;match_all&quot;: {} } } . Compare the result here with the result you got previous time running this query (for individual documents). You’ll only get top 10 results from this query. . . Queries in Elasticsearch . Boolean . Let’s start with Boolean queries. In the boolean query, there are four kinds of occurrences: must, should, must_not and filter. must mimics the boolean “AND”. For example, if we are searching for documents with “case-base” in content and with “deep learning” in title, try the bool DSL query: . GET documents/_search { &quot;query&quot; : { &quot;bool&quot; : { &quot;must&quot;: [{ &quot;match&quot;: { &quot;content&quot;: &quot;case-based&quot; } }, { &quot;match&quot;: { &quot;title&quot;: &quot;deep learning&quot; } }] } } } . Try describing this query in Boolean Logic. . The functionality of should somewhat corresponds to the Boolean “OR”. Let’s look at an example, assume you want to search for either “NLP” or “deep learning” in the title, and if either of the criteria matches the document should be returned with the result. The query might look like: . GET documents/_search { &quot;query&quot; : { &quot;bool&quot; : { &quot;should&quot;: [{ &quot;match&quot;: { &quot;title&quot;: &quot;deep learning&quot; } }, { &quot;match&quot;: { &quot;title&quot;: &quot;nlp&quot; } }] } } } . Again try describing this query in Boolean Logic. . Try some queries using should and filter. Note filter can be used effectively with numeric data but is also quite similar to must, in that the term has to appear in the document; however, they do not contribute to the score. . Basic information on search is available at https://www.elastic.co/guide/en/elasticsearch/reference/7.0/getting-started-search.html . Ranked queries with BM25 . It is easy to perform ranked queries. The scores are based on the BM25 algorithm by default using the standard analyser (however, you can change to alternative models by setting up a custom analyser, see Appendix for more details). . For example, a ranked query for documents about “deep learning” can be written as: . GET documents/_search { &quot;query&quot;: {&quot;match&quot;: {&quot;content&quot;: &quot;deep learning&quot;}} } . Note the documents are ranked by score: . Try this query but replace “match” with “match_phrase”. Compare the results. | Now try one of the queries from the completed AnalysisHandout sheet from the coursework. Compare the results from tf, idf, and tf*idf with those obtained here. | . We have just touched the surface of the functionality available in ElasticSearch today. You will find more detailed information about Elasticsearch at this link. . Appendix-Analyzers . You can add different types of analyzers in your index in Elasticsearch. For example, applying a stop word removal on your index. . To define an analyser in an index, we need to first define the mappings here. For this, run the following command on Kibana console: . PUT documents/ { &quot;settings&quot;: { &quot;analysis&quot;: { &quot;analyzer&quot;: { &quot;my_analyzer&quot;: { &quot;type&quot;: &quot;stop&quot;, &quot;stopwords&quot;: &quot;_english_&quot; } } } }, &quot;mappings&quot;: { &quot;properties&quot;: { &quot;title&quot;: { &quot;type&quot;: &quot;text&quot; }, &quot;content&quot;: { &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;my_analyzer&quot; } } } } . Make sure there is no other existing index with name documnets. Now let’s add some data to the index documents. Open a command prompt, navigate to the folder where the JSON file name all_docs.json (from indexing example) is saved. Run the following command to add the JSON to our index. . curl -H &quot;Content-Type: application/x-ndjson&quot; -XPOST &quot;localhost:9200/documents/_bulk?pretty&quot; --data-binary @all_docs.json . Go to Kibana console and run following command to verify the indexing: . GET documents/_search { &quot;query&quot;: { &quot;match_all&quot;: {} } } . You should see the indexed files here. . Now to verify the stop-word removal from indexing run the following command: . GET /documents/_search { &quot;query&quot;: { &quot;match&quot;: { &quot;content&quot;: &quot;the&quot; } } } . You should see a result with 0 hits, because word “the” is not indexed here. . You can also customize the analyzers according to your use. Let’s say you want to apply an html filter as an analyser. You would need to create new mapping on different index. Changing the mapping of an existing field could invalidate data that’s already indexed. If you need to change the mapping of a field, create a new index with the correct mappings and re-index your data into that index. . Let’s make a different index named irdocuments. Run the following command: . PUT irdocuments/ { &quot;settings&quot;: { &quot;analysis&quot;: { &quot;analyzer&quot;: { &quot;blogs_analyzer&quot;: { &quot;type&quot;: &quot;custom&quot;, &quot;tokenizer&quot;: &quot;standard&quot;, &quot;stopwords&quot;: &quot;_english_&quot;, &quot;char_filter&quot;: &quot;html_strip&quot;, &quot;filter&quot;: [ &quot;stop&quot;, &quot;lowercase&quot;, &quot;asciifolding&quot; ] } } } }, &quot;mappings&quot;: { &quot;properties&quot;: { &quot;title&quot;: { &quot;type&quot;: &quot;text&quot; }, &quot;content&quot;: { &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;blogs_analyzer&quot; } } } } . Let’s run an example query to verify our analyser: . POST irdocuments/_analyze { &quot;analyzer&quot;: &quot;blogs_analyzer&quot;, &quot;text&quot;: &quot;is this the &lt;b&gt;déjà vu&lt;/b&gt;?.&quot; } . The result should show only two words: [deja, vu] . . Appendix-Py-Script . import json, os, re txt_files = &quot;C: Users User Downloads CourseworkResourses CourseworkResourses Documents &quot; ctr = 1 j = [] for txt in os.listdir(txt_files): s1 = &quot;&quot; s2 = &quot;&quot; print(ctr) f = open(txt_files + txt, &quot;r&quot;) l = f.readlines() lines = list(filter(lambda x: not x.isspace(), l)) t = lines[2] tt = t.strip(&#39; n&#39;) s1 = &quot;{ &quot;index &quot;:{ &quot;_id &quot;:&quot; + &quot; &quot;&quot; + str(ctr) + &quot; &quot;&quot; + &quot;}}&quot; ctr += 1 for i in lines: if i[0] == &quot;#&quot;: ii = i.replace(&#39;&quot;&#39;, &#39;&#39;) iii = ii.strip(&#39; n&#39;) iv = iii.replace(&#39;#&#39;, &#39;&#39;) s2 = &quot;{ &quot;title &quot;: &quot;&quot; + tt + &quot; &quot;, &quot;content &quot;:&quot; + &quot; &quot;&quot; + str(iv) + &quot; &quot;&quot; + &quot;}&quot; with open(&quot;all_docs.json&quot;, &quot;a&quot;) as the_file: the_file.write(s1 + &quot; n&quot;) the_file.write(s2 + &quot; n&quot;) .",
            "url": "https://ashishu007.github.io/nlp-blog/markdown/ir/elasticsearch/kibana/2020/11/23/es-kb.html",
            "relUrl": "/markdown/ir/elasticsearch/kibana/2020/11/23/es-kb.html",
            "date": " • Nov 23, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Semantic Representations",
            "content": "Introduction . Word Embeddings are used to represent words in a multi-dimensional vector form. A word wi in vocabulary V is represented in the form of a vector of n dimensions. These vectors are generated by unsupervised training on a large corpus of words to gain the semantic similarities between them. . Downloading the required vector file . import sys . !mkdir vectors # Download the different files using these commands. This may take a while !cd vectors &amp;&amp; curl -O http://magnitude.plasticity.ai/word2vec/light/GoogleNews-vectors-negative300.magnitude # !cd vectors &amp;&amp; curl -O http://magnitude.plasticity.ai/glove/light/glove.6B.50d.magnitude !cd vectors &amp;&amp; curl -O http://magnitude.plasticity.ai/elmo/light/elmo_2x1024_128_2048cnn_1xhighway_weights.magnitude . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 4016M 100 4016M 0 0 39.7M 0 0:01:40 0:01:40 --:--:-- 44.9M % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 51.9M 100 51.9M 0 0 43.4M 0 0:00:01 0:00:01 --:--:-- 43.4M . !ls !cd vectors &amp;&amp; ls . sample_data vectors elmo_2x1024_128_2048cnn_1xhighway_weights.magnitude GoogleNews-vectors-negative300.magnitude . Install the required libraries. Again this may take a while . !pip3 install torch numpy scikit-learn pandas transformers==3.1.0 seaborn matplotlib sentence_transformers . Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.6.0+cu101) Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5) Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.22.2.post1) Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.1.3) Collecting transformers==3.1.0 Downloading https://files.pythonhosted.org/packages/ae/05/c8c55b600308dc04e95100dc8ad8a244dd800fe75dfafcf1d6348c6f6209/transformers-3.1.0-py3-none-any.whl (884kB) |████████████████████████████████| 890kB 8.1MB/s Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (0.11.0) Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.2) Collecting sentence_transformers Downloading https://files.pythonhosted.org/packages/f4/fd/0190080aa0af78d7cd5874e4e8e85f0bed9967dd387cf05d760832b95da9/sentence-transformers-0.3.8.tar.gz (66kB) |████████████████████████████████| 71kB 9.9MB/s Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0) Requirement already satisfied: scipy&gt;=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1) Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.17.0) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (2.23.0) Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (20.4) Collecting sentencepiece!=0.1.92 Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB) |████████████████████████████████| 1.1MB 14.7MB/s Collecting tokenizers==0.8.1.rc2 Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB) |████████████████████████████████| 3.0MB 52.3MB/s Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (2019.12.20) Requirement already satisfied: dataclasses; python_version &lt; &#34;3.7&#34; in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (0.7) Requirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (4.41.1) Collecting sacremoses Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB) |████████████████████████████████| 890kB 51.7MB/s Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (3.0.12) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.2.0) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0) Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from sentence_transformers) (3.2.5) Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil&gt;=2.7.3-&gt;pandas) (1.15.0) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers==3.1.0) (3.0.4) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers==3.1.0) (1.24.3) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers==3.1.0) (2020.6.20) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers==3.1.0) (2.10) Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers==3.1.0) (7.1.2) Building wheels for collected packages: sentence-transformers, sacremoses Building wheel for sentence-transformers (setup.py) ... done Created wheel for sentence-transformers: filename=sentence_transformers-0.3.8-cp36-none-any.whl size=101996 sha256=d88a5a734040b3392f98e3da764628774ab81a202c9ec9de5fa0ed0904c70db3 Stored in directory: /root/.cache/pip/wheels/27/ec/b3/d12cc8e4daf77846db6543033d3a5642f204c0320b15945647 Building wheel for sacremoses (setup.py) ... done Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=edeeea45ac8dc6966f84571553917635487c3d6152e4d40457476b9f95d14906 Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45 Successfully built sentence-transformers sacremoses Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers, sentence-transformers Successfully installed sacremoses-0.0.43 sentence-transformers-0.3.8 sentencepiece-0.1.94 tokenizers-0.8.1rc2 transformers-3.1.0 . import torch torch.__version__ . &#39;1.6.0+cu101&#39; . Since pymagnitude can not installed properly with pypi, we&#39;ll use another method for it. Install Magnitude on Google Colab . ! echo &quot;Installing Magnitude.... (please wait, can take a while)&quot; ! (curl https://raw.githubusercontent.com/plasticityai/magnitude/master/install-colab.sh | /bin/bash 1&gt;/dev/null 2&gt;/dev/null) ! echo &quot;Done installing Magnitude.&quot; . Installing Magnitude.... (please wait, can take a while) % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 137 100 137 0 0 1122 0 --:--:-- --:--:-- --:--:-- 1122 Done installing Magnitude. . Import some libraries . import pymagnitude as pym import numpy as np import seaborn as sns import pandas as pd from sklearn.metrics.pairwise import cosine_similarity import matplotlib.pyplot as plt . def similarity_between_docs(doc1, doc2, is_1d=False): if is_1d: v1 = np.reshape(doc1, (1, -1)) v2 = np.reshape(doc2, (1, -1)) else: d1 = np.mean(doc1, axis=0) d2 = np.mean(doc2, axis=0) v1 = np.reshape(d1, (1, -1)) v2 = np.reshape(d2, (1, -1)) return cosine_similarity(v1, v2)[0][0] def plot_1d_heatmap(vec, name): v = vec.reshape(1, -1) plt.figure(figsize=(20, 2)) sns.heatmap(v, cmap=&quot;YlGnBu&quot;).set_title(name) plt.rcParams.update({&quot;font.size&quot;: 22}) plt.show() return . Static Word Embeddings . Static WEs have fixed vector value for each word. They loose the contextual information . Google&#39;s Word2Vec | Stanford&#39;s GloVe | Facebook&#39;s fastText | Google&#39;s Word2Vec . # glove_vectors = pym.Magnitude(&quot;./vectors/glove.6B.50d.magnitude&quot;) w2v_vectors = pym.Magnitude(&quot;./vectors/GoogleNews-vectors-negative300.magnitude&quot;) . For using other vectors, download the pre-trained vectors from pymagnitude repo and put them in ./vectors folder . print(&quot;Vector Name: {} nTotal words: {} nDimension of each word: {}&quot;. format(&quot;Word2Vec&quot;, len(w2v_vectors), w2v_vectors.dim)) . Vector Name: Word2Vec Total words: 3000000 Dimension of each word: 300 . for i, (key, vec) in enumerate(w2v_vectors): if i == 1000: print(&quot;Index = {} Word: {} nVector Size: {} nVector: {}&quot;.format(i, key, vec.shape, vec)) break . Index = 1000 Word: Paul Vector Size: (300,) Vector: [ 1.211609e-01 4.324040e-02 -5.569700e-03 8.384690e-02 2.085200e-02 -1.558410e-02 -4.038700e-02 -9.965050e-02 -3.380210e-02 -2.205920e-02 -5.531260e-02 -2.688810e-02 7.155520e-02 8.340790e-02 7.133600e-03 3.533860e-02 1.167710e-01 4.324040e-02 7.901800e-02 8.296890e-02 -4.411840e-02 1.525490e-02 8.472480e-02 -6.277540e-02 3.923500e-03 -1.234660e-02 3.402160e-02 6.101940e-02 1.942530e-02 2.151050e-02 -4.828880e-02 2.030320e-02 3.292420e-02 -5.882450e-02 6.453130e-02 -2.611980e-02 6.063500e-03 1.439883e-01 -3.731400e-02 1.064550e-02 1.251120e-02 -2.074220e-02 -1.679130e-02 -7.243310e-02 -7.199420e-02 -2.359560e-02 1.920580e-02 -2.677830e-02 5.443460e-02 3.468010e-02 2.370540e-02 -5.679400e-03 -2.238840e-02 -9.526060e-02 -8.724900e-03 5.202020e-02 2.963170e-02 -9.877250e-02 3.380210e-02 -5.487360e-02 6.233640e-02 -1.190760e-02 -9.306560e-02 -4.192340e-02 -8.296890e-02 -8.428580e-02 -2.513210e-02 -6.936020e-02 -1.525490e-02 2.019350e-02 2.238840e-02 -1.255508e-01 2.293720e-02 -1.712060e-02 -3.270470e-02 2.118120e-02 -7.901800e-02 4.148440e-02 8.472480e-02 4.697180e-02 7.243310e-02 -3.819200e-02 7.989600e-02 -8.604180e-02 5.443460e-02 -4.960570e-02 -6.672630e-02 1.290627e-01 7.111620e-02 -6.101940e-02 6.628730e-02 6.008700e-03 8.428580e-02 -6.173300e-03 -2.919280e-02 6.392800e-03 -2.809530e-02 -4.609380e-02 -4.806930e-02 -9.306560e-02 -5.706850e-02 1.569380e-02 9.569950e-02 -1.799850e-02 1.295000e-04 -7.726200e-02 7.945700e-02 -1.591330e-02 3.577760e-02 6.760430e-02 -1.723030e-02 -2.826000e-03 -9.218760e-02 1.481590e-02 1.322450e-02 -1.646200e-03 -7.155520e-02 -3.841150e-02 3.797250e-02 6.453130e-02 -7.901800e-02 -3.566800e-03 -2.480290e-02 -1.633038e-01 -4.763030e-02 -9.921150e-02 -5.750750e-02 3.292420e-02 3.753350e-02 5.114220e-02 -3.753350e-02 -1.119421e-01 8.121300e-03 -9.306560e-02 4.148440e-02 5.624500e-03 -4.389900e-03 7.111620e-02 2.469310e-02 4.302090e-02 3.072900e-03 5.882450e-02 1.262090e-02 -5.662950e-02 -6.716530e-02 -2.387000e-03 1.022844e-01 -1.299407e-01 -7.550610e-02 2.030320e-02 2.480290e-02 -8.340790e-02 1.598200e-03 1.290627e-01 6.804330e-02 4.192340e-02 -2.622960e-02 4.784980e-02 -3.336310e-02 2.271770e-02 -1.262090e-02 3.907000e-02 1.132591e-01 -8.823670e-02 -7.737200e-03 3.336310e-02 -6.804330e-02 -2.579060e-02 2.831480e-02 -6.447600e-03 -4.016750e-02 2.601010e-02 2.052270e-02 -1.150151e-01 1.093082e-01 2.941220e-02 -1.810830e-02 2.260790e-02 -9.109000e-03 2.754650e-02 -6.277540e-02 -4.236240e-02 1.659900e-03 -5.377600e-03 -3.885050e-02 -3.928950e-02 -1.037110e-02 -8.604180e-02 7.989600e-02 -6.145840e-02 -6.540930e-02 4.280100e-03 -6.892120e-02 -2.831480e-02 5.679400e-03 5.070320e-02 6.189740e-02 5.421510e-02 -2.151050e-02 -2.370540e-02 5.882450e-02 8.735880e-02 3.621660e-02 5.421510e-02 1.141371e-01 2.194940e-02 5.092270e-02 1.255508e-01 -7.594510e-02 -2.513210e-02 2.096170e-02 -1.264288e-01 8.121290e-02 -4.455740e-02 -1.022844e-01 -7.331110e-02 -4.345990e-02 -2.721730e-02 6.760430e-02 -8.472480e-02 1.545240e-01 -8.472480e-02 1.141371e-01 -1.093082e-01 -1.668160e-02 -7.846900e-03 2.784800e-03 4.631330e-02 -2.633930e-02 -1.036013e-01 -3.928950e-02 1.887650e-02 -3.753350e-02 -3.621660e-02 6.804330e-02 -4.653280e-02 6.233640e-02 -8.604180e-02 1.037110e-02 -3.446060e-02 3.577760e-02 4.236240e-02 -8.560300e-03 -2.809530e-02 3.270470e-02 7.155520e-02 -2.976900e-03 -1.152350e-02 4.938620e-02 -1.395984e-01 -2.897330e-02 1.388300e-02 -5.750750e-02 3.533860e-02 4.960570e-02 4.675230e-02 7.243300e-03 -3.358260e-02 -1.152300e-03 3.775300e-02 5.750750e-02 5.202020e-02 7.792100e-03 2.831480e-02 4.148440e-02 6.101940e-02 4.993500e-03 -1.059060e-02 -1.481590e-02 -3.402160e-02 3.380210e-02 -3.468010e-02 -5.882450e-02 6.189740e-02 -5.882450e-02 -8.560280e-02 2.140070e-02 5.882450e-02 1.152300e-03 -5.289810e-02 -2.260790e-02 6.233640e-02 2.875380e-02 7.287210e-02 -6.447600e-03 1.141371e-01 1.613280e-02 -5.662950e-02 8.033490e-02 -2.249820e-02 5.953800e-03 -4.126490e-02 3.950900e-02 3.072920e-02 -6.540930e-02 -7.989600e-02 4.653280e-02 -8.999300e-03 -6.557400e-03 4.938600e-03] . print(w2v_vectors.query(&quot;dog&quot;)) # Get the vector using the index print(w2v_vectors[1000]) . [ 1.719810e-02 -7.493400e-03 -5.798200e-02 5.405100e-02 -2.833580e-02 1.924540e-02 1.965490e-02 -2.768070e-02 -5.159400e-03 -2.129280e-02 6.027510e-02 -1.421706e-01 -7.575300e-03 -5.568890e-02 -8.435200e-03 3.603400e-02 -6.682670e-02 5.339590e-02 -6.289580e-02 -4.029260e-02 5.208550e-02 -3.324960e-02 4.782700e-02 -5.503380e-02 -2.997380e-02 6.715430e-02 -5.012010e-02 1.074469e-01 1.100676e-01 8.189600e-03 -3.259440e-02 -2.751690e-02 -1.220240e-02 -2.882720e-02 -3.308580e-02 2.610400e-03 -4.504300e-03 1.768940e-02 4.979250e-02 1.120331e-01 5.568900e-03 -7.141290e-02 -5.057000e-03 1.760750e-02 -3.603400e-02 -2.981000e-02 8.353340e-02 -2.358590e-02 -5.364200e-03 2.538760e-02 -2.358590e-02 3.996500e-02 7.698180e-02 4.749900e-03 3.865470e-02 2.518300e-03 9.237810e-02 -8.189550e-02 9.958490e-02 1.171110e-02 8.124030e-02 4.553390e-02 4.782700e-02 5.896500e-03 9.827500e-03 -4.078400e-02 7.657200e-03 -1.596960e-02 -5.208550e-02 1.054400e-03 1.159640e-01 4.111150e-02 -6.551640e-02 2.718930e-02 -2.293070e-02 -4.934200e-03 7.206810e-02 -4.062020e-02 5.274070e-02 -6.944740e-02 4.586150e-02 -4.356840e-02 1.777130e-02 -9.106780e-02 -1.002401e-01 -6.191300e-02 -7.698180e-02 3.996500e-02 5.138900e-03 -8.779200e-02 -4.127530e-02 -6.265000e-03 -2.178420e-02 -2.735310e-02 2.637040e-02 -1.185847e-01 1.760750e-02 -8.230500e-03 -1.822200e-03 -7.010260e-02 -7.075770e-02 -6.027510e-02 8.124030e-02 8.648170e-02 4.618910e-02 -7.075770e-02 -7.288700e-03 -4.651660e-02 6.183100e-03 -4.156200e-03 -5.339590e-02 5.405100e-02 6.977500e-02 3.455990e-02 3.292200e-02 -2.293070e-02 -2.927800e-03 -9.696430e-02 -7.206810e-02 -3.832710e-02 -7.436110e-02 1.384030e-02 -1.048262e-01 -1.875410e-02 -3.275820e-02 1.949110e-02 -1.359470e-02 -5.830960e-02 5.503380e-02 -8.517130e-02 -5.175800e-02 -7.780100e-03 -7.993000e-02 6.961100e-03 -9.172300e-02 1.310300e-03 3.816330e-02 -5.830960e-02 8.648170e-02 7.894730e-02 1.752560e-02 2.293070e-02 -5.896480e-02 5.372350e-02 -2.009000e-04 2.006440e-02 -7.108530e-02 -1.859030e-02 -2.522380e-02 -1.028608e-01 1.434809e-01 1.785320e-02 -7.010260e-02 -1.916350e-02 -7.043010e-02 1.105590e-02 3.537890e-02 -5.044760e-02 -3.144790e-02 3.914610e-02 2.162040e-02 9.418000e-03 8.091280e-02 -4.225810e-02 -3.374090e-02 -4.115200e-03 -1.094000e-04 5.323200e-03 4.291320e-02 -1.113780e-02 1.367660e-02 -4.422360e-02 3.292200e-02 5.863720e-02 -7.927490e-02 1.736180e-02 6.158540e-02 8.148600e-03 -1.447913e-01 8.255070e-02 -1.019600e-02 -8.312400e-03 -3.930980e-02 5.405100e-02 -1.916350e-02 3.910500e-03 9.434360e-02 1.434809e-01 1.531450e-02 3.390470e-02 -1.326710e-02 5.937400e-03 -3.013750e-02 4.553390e-02 6.977500e-02 6.322330e-02 -5.110280e-02 -7.960240e-02 -6.387850e-02 2.391350e-02 -8.255070e-02 -8.779200e-02 -7.861970e-02 -4.880970e-02 -3.931000e-03 -5.044760e-02 -3.799950e-02 6.125780e-02 8.844720e-02 -4.618910e-02 -1.539636e-01 -1.572390e-02 -4.258570e-02 -1.416790e-02 -5.601650e-02 4.258570e-02 8.713680e-02 -8.189550e-02 -7.370600e-02 -2.915480e-02 5.339590e-02 -1.269380e-02 3.009700e-03 -9.303330e-02 -3.521510e-02 -5.896480e-02 7.665420e-02 -9.090400e-03 9.565400e-02 -9.172300e-02 5.405100e-03 1.981870e-02 -8.025760e-02 5.961990e-02 -4.520630e-02 4.651660e-02 1.185847e-01 4.094780e-02 4.815460e-02 3.095650e-02 7.698180e-02 -1.008953e-01 -1.637910e-02 -6.027510e-02 9.958490e-02 5.896480e-02 1.613340e-02 -1.136300e-03 2.653410e-02 -7.993000e-02 -7.763690e-02 5.568890e-02 -7.174050e-02 -2.358590e-02 -2.538760e-02 6.584400e-02 -4.356840e-02 -3.554270e-02 -1.185847e-01 -3.914610e-02 -1.711620e-02 1.138350e-02 -4.815460e-02 -1.310300e-03 5.830960e-02 -3.341340e-02 -5.568890e-02 -2.866340e-02 -1.284122e-01 1.981870e-02 -2.088340e-02 2.964620e-02 -2.981000e-02 1.100676e-01 2.293070e-02 -6.420610e-02 -2.802000e-04 3.488750e-02 5.110280e-02 -5.144000e-04 1.395500e-01 -1.113780e-02 5.012010e-02 8.124030e-02 -5.929230e-02 -1.654290e-02 -4.176670e-02 4.225810e-02 5.863720e-02 9.434360e-02 -6.060270e-02 3.455990e-02 -9.237810e-02 8.779200e-02 8.255070e-02 -1.580580e-02 2.096530e-02 1.395500e-01 -1.192399e-01 7.468870e-02] (&#39;Paul&#39;, array([ 1.211609e-01, 4.324040e-02, -5.569700e-03, 8.384690e-02, 2.085200e-02, -1.558410e-02, -4.038700e-02, -9.965050e-02, -3.380210e-02, -2.205920e-02, -5.531260e-02, -2.688810e-02, 7.155520e-02, 8.340790e-02, 7.133600e-03, 3.533860e-02, 1.167710e-01, 4.324040e-02, 7.901800e-02, 8.296890e-02, -4.411840e-02, 1.525490e-02, 8.472480e-02, -6.277540e-02, 3.923500e-03, -1.234660e-02, 3.402160e-02, 6.101940e-02, 1.942530e-02, 2.151050e-02, -4.828880e-02, 2.030320e-02, 3.292420e-02, -5.882450e-02, 6.453130e-02, -2.611980e-02, 6.063500e-03, 1.439883e-01, -3.731400e-02, 1.064550e-02, 1.251120e-02, -2.074220e-02, -1.679130e-02, -7.243310e-02, -7.199420e-02, -2.359560e-02, 1.920580e-02, -2.677830e-02, 5.443460e-02, 3.468010e-02, 2.370540e-02, -5.679400e-03, -2.238840e-02, -9.526060e-02, -8.724900e-03, 5.202020e-02, 2.963170e-02, -9.877250e-02, 3.380210e-02, -5.487360e-02, 6.233640e-02, -1.190760e-02, -9.306560e-02, -4.192340e-02, -8.296890e-02, -8.428580e-02, -2.513210e-02, -6.936020e-02, -1.525490e-02, 2.019350e-02, 2.238840e-02, -1.255508e-01, 2.293720e-02, -1.712060e-02, -3.270470e-02, 2.118120e-02, -7.901800e-02, 4.148440e-02, 8.472480e-02, 4.697180e-02, 7.243310e-02, -3.819200e-02, 7.989600e-02, -8.604180e-02, 5.443460e-02, -4.960570e-02, -6.672630e-02, 1.290627e-01, 7.111620e-02, -6.101940e-02, 6.628730e-02, 6.008700e-03, 8.428580e-02, -6.173300e-03, -2.919280e-02, 6.392800e-03, -2.809530e-02, -4.609380e-02, -4.806930e-02, -9.306560e-02, -5.706850e-02, 1.569380e-02, 9.569950e-02, -1.799850e-02, 1.295000e-04, -7.726200e-02, 7.945700e-02, -1.591330e-02, 3.577760e-02, 6.760430e-02, -1.723030e-02, -2.826000e-03, -9.218760e-02, 1.481590e-02, 1.322450e-02, -1.646200e-03, -7.155520e-02, -3.841150e-02, 3.797250e-02, 6.453130e-02, -7.901800e-02, -3.566800e-03, -2.480290e-02, -1.633038e-01, -4.763030e-02, -9.921150e-02, -5.750750e-02, 3.292420e-02, 3.753350e-02, 5.114220e-02, -3.753350e-02, -1.119421e-01, 8.121300e-03, -9.306560e-02, 4.148440e-02, 5.624500e-03, -4.389900e-03, 7.111620e-02, 2.469310e-02, 4.302090e-02, 3.072900e-03, 5.882450e-02, 1.262090e-02, -5.662950e-02, -6.716530e-02, -2.387000e-03, 1.022844e-01, -1.299407e-01, -7.550610e-02, 2.030320e-02, 2.480290e-02, -8.340790e-02, 1.598200e-03, 1.290627e-01, 6.804330e-02, 4.192340e-02, -2.622960e-02, 4.784980e-02, -3.336310e-02, 2.271770e-02, -1.262090e-02, 3.907000e-02, 1.132591e-01, -8.823670e-02, -7.737200e-03, 3.336310e-02, -6.804330e-02, -2.579060e-02, 2.831480e-02, -6.447600e-03, -4.016750e-02, 2.601010e-02, 2.052270e-02, -1.150151e-01, 1.093082e-01, 2.941220e-02, -1.810830e-02, 2.260790e-02, -9.109000e-03, 2.754650e-02, -6.277540e-02, -4.236240e-02, 1.659900e-03, -5.377600e-03, -3.885050e-02, -3.928950e-02, -1.037110e-02, -8.604180e-02, 7.989600e-02, -6.145840e-02, -6.540930e-02, 4.280100e-03, -6.892120e-02, -2.831480e-02, 5.679400e-03, 5.070320e-02, 6.189740e-02, 5.421510e-02, -2.151050e-02, -2.370540e-02, 5.882450e-02, 8.735880e-02, 3.621660e-02, 5.421510e-02, 1.141371e-01, 2.194940e-02, 5.092270e-02, 1.255508e-01, -7.594510e-02, -2.513210e-02, 2.096170e-02, -1.264288e-01, 8.121290e-02, -4.455740e-02, -1.022844e-01, -7.331110e-02, -4.345990e-02, -2.721730e-02, 6.760430e-02, -8.472480e-02, 1.545240e-01, -8.472480e-02, 1.141371e-01, -1.093082e-01, -1.668160e-02, -7.846900e-03, 2.784800e-03, 4.631330e-02, -2.633930e-02, -1.036013e-01, -3.928950e-02, 1.887650e-02, -3.753350e-02, -3.621660e-02, 6.804330e-02, -4.653280e-02, 6.233640e-02, -8.604180e-02, 1.037110e-02, -3.446060e-02, 3.577760e-02, 4.236240e-02, -8.560300e-03, -2.809530e-02, 3.270470e-02, 7.155520e-02, -2.976900e-03, -1.152350e-02, 4.938620e-02, -1.395984e-01, -2.897330e-02, 1.388300e-02, -5.750750e-02, 3.533860e-02, 4.960570e-02, 4.675230e-02, 7.243300e-03, -3.358260e-02, -1.152300e-03, 3.775300e-02, 5.750750e-02, 5.202020e-02, 7.792100e-03, 2.831480e-02, 4.148440e-02, 6.101940e-02, 4.993500e-03, -1.059060e-02, -1.481590e-02, -3.402160e-02, 3.380210e-02, -3.468010e-02, -5.882450e-02, 6.189740e-02, -5.882450e-02, -8.560280e-02, 2.140070e-02, 5.882450e-02, 1.152300e-03, -5.289810e-02, -2.260790e-02, 6.233640e-02, 2.875380e-02, 7.287210e-02, -6.447600e-03, 1.141371e-01, 1.613280e-02, -5.662950e-02, 8.033490e-02, -2.249820e-02, 5.953800e-03, -4.126490e-02, 3.950900e-02, 3.072920e-02, -6.540930e-02, -7.989600e-02, 4.653280e-02, -8.999300e-03, -6.557400e-03, 4.938600e-03], dtype=float32)) . doc_vecs = w2v_vectors.query([&quot;I&quot;, &quot;read&quot;, &quot;a&quot;, &quot;book&quot;]) doc_vecs.shape . (4, 300) . mul_doc_vecs = w2v_vectors.query([[&quot;I&quot;, &quot;read&quot;, &quot;a&quot;, &quot;book&quot;], [&quot;I&quot;, &quot;read&quot;, &quot;a&quot;, &quot;sports&quot;, &quot;magazine&quot;]]) mul_doc_vecs.shape . (2, 5, 300) . print(&quot;Similarity between &quot;Apple &quot; and &quot;Mango &quot;: {}&quot;. format(w2v_vectors.similarity(&quot;apple&quot;, &quot;mango&quot;))) print(&quot;Similarity between &quot;Apple &quot; and [ &quot;Mango &quot;, &quot;Orange &quot;]: {}&quot;. format(w2v_vectors.similarity(&quot;apple&quot;, [&quot;mango&quot;, &quot;orange&quot;]))) print(&quot;Most similar to &quot;Cat &quot; from [ &quot;Dog &quot;, &quot;Television &quot;, &quot;Laptop &quot;]: {}&quot;. format(w2v_vectors.most_similar_to_given(&quot;cat&quot;, [&quot;dog&quot;, &quot;television&quot;, &quot;laptop&quot;]))) . Similarity between &#34;Apple&#34; and &#34;Mango&#34;: 0.5751857161521912 Similarity between &#34;Apple&#34; and [&#34;Mango&#34;, &#34;Orange&#34;]: [0.5751857, 0.39203462] Most similar to &#34;Cat&#34; from [&#34;Dog&#34;, &#34;Television&#34;, &#34;Laptop&#34;]: dog . doc1 = w2v_vectors.query([&quot;I&quot;, &quot;read&quot;, &quot;a&quot;, &quot;book&quot;]) doc2 = w2v_vectors.query([&quot;I&quot;, &quot;read&quot;, &quot;a&quot;, &quot;sports&quot;, &quot;magazine&quot;]) print(&quot;Similarity between n &quot;I read a book &quot; and &quot;I read a sports magazine &quot;: {}&quot;. format(similarity_between_docs(doc1, doc2, is_1d=False))) . Similarity between &#34;I read a book&#34; and &#34;I read a sports magazine&#34;: 0.8234725594520569 . plot_1d_heatmap(w2v_vectors.query(&quot;king&quot;), &quot;King&quot;) plot_1d_heatmap(w2v_vectors.query(&quot;man&quot;), &quot;Man&quot;) plot_1d_heatmap(w2v_vectors.query(&quot;woman&quot;), &quot;Woman&quot;) plot_1d_heatmap(w2v_vectors.query(&quot;queen&quot;), &quot;Queen&quot;) tmp = w2v_vectors.query(&quot;king&quot;) - w2v_vectors.query(&quot;man&quot;) + w2v_vectors.query(&quot;woman&quot;) plot_1d_heatmap(tmp, &quot;King - Man + Woman&quot;) print(&quot;Similarity between n &quot;King - Man + Woman &quot; and &quot;Queen &quot;: {}&quot;. format(similarity_between_docs(tmp, w2v_vectors.query(&quot;queen&quot;), is_1d=True))) . Similarity between &#34;King - Man + Woman&#34; and &#34;Queen&#34;: 0.7118194103240967 . Exercises . It&#39;s time for you to explore these functionalities. . Try some to write code for some of the queries below. Or head on to this GitHub repository to have a look at different examples: . https://github.com/plasticityai/magnitude#querying . &quot;&quot;&quot; Try plotting heatmap for four different words, out of which three should have same property and should be different. For example, &quot;girl&quot;, &quot;boy&quot;, &quot;man&quot;, &quot;water &quot;&quot;&quot; # Your code here . &#39; nTry plotting heatmap for four different words, out of which three should have nsame property and should be different. For example, &#34;girl&#34;, &#34;boy&#34;, &#34;man&#34;, &#34;water n&#39; . &quot;&quot;&quot; Calculate the similarity between two words with similar sense and two with no similarity. For example, similarity between &quot;cat&quot; &amp; &quot;dog&quot; and similarity between &quot;apple&quot; and &quot;lion&quot; &quot;&quot;&quot; # Your code here . &#39; nCalculate the similarity between two words with similar sense and two with no nsimilarity. For example, similarity between &#34;cat&#34; &amp; &#34;dog&#34; and similarity between n&#34;apple&#34; and &#34;lion&#34; n&#39; . &quot;&quot;&quot; Print the similarity score of &quot;paris&quot; with following: &quot;delhi&quot;, &quot;vienna&quot;, &quot;london&quot;, france, &quot;laptop&quot; &quot;&quot;&quot; # Your code here . &#39; nPrint the similarity score of &#34;paris&#34; with following: n&#34;delhi&#34;, &#34;vienna&#34;, &#34;london&#34;, france, &#34;laptop&#34; n&#39; . Contextual Word Embeddings . These algoirtms also take the context of the word in some sentence while generating the Embeddings . AllenAI&#39;s ELMo | Google&#39;s BERT | AllenAI&#39;s ELMo . elmo_vecs = pym.Magnitude(&#39;./vectors/elmo_2x1024_128_2048cnn_1xhighway_weights.magnitude&#39;) . ELMo generates embedding of a word based on its context. So we need to provide a full sentence in order to get the embedding of some word. . sen1 = elmo_vecs.query([&quot;yes&quot;, &quot;they&quot;, &quot;are&quot;, &quot;right&quot;]) sen2 = elmo_vecs.query([&quot;go&quot;, &quot;to&quot;, &quot;your&quot;, &quot;right&quot;]) . /usr/local/lib/python3.6/dist-packages/pymagnitude/third_party/allennlp/nn/util.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor). index_range = sequence_lengths.new_tensor(torch.arange(0, len(sequence_lengths))) . . right1 = sen1[-1] right2 = sen2[-1] print(&quot;right from sentence 1: {} tright from sentence 2: {}&quot;.format(right1.shape, right2.shape)) . right from sentence 1: (768,) right from sentence 2: (768,) . plot_1d_heatmap(right1, name=&quot;ELMo vec for right in &quot;yes they are right &quot;&quot;) plot_1d_heatmap(right2, name=&quot;ELMo vec for right in &quot;go to your right &quot;&quot;) . print(&quot;Simialrity between &quot;right &quot; from sentence 1 &amp; 2: t{}&quot;. format(similarity_between_docs(right1, right2, is_1d=True))) print(&quot;Simialrity between &quot;right &quot; from sentence 1 only: t{}&quot;. format(similarity_between_docs(right1, right1, is_1d=True))) print(&quot;Simialrity between &quot;right &quot; from sentence 2 only: t{}&quot;. format(similarity_between_docs(right2, right2, is_1d=True))) . Simialrity between &#34;right&#34; from sentence 1 &amp; 2: 0.7283329963684082 Simialrity between &#34;right&#34; from sentence 1 only: 0.9999997019767761 Simialrity between &#34;right&#34; from sentence 2 only: 1.000000238418579 . Google&#39;s BERT . Since pymagnitude doesn&#39;t have support for BERT yet, we&#39;ll use the huggingface&#39;s transfomers library for this. . import torch import transformers . # this may take a while for first time model_class, tokenizer_class, pretrained_weights = (transformers.BertModel, transformers.BertTokenizer, &#39;bert-base-uncased&#39;) # Load pretrained model/tokenizer tokenizer = tokenizer_class.from_pretrained(pretrained_weights) model = model_class.from_pretrained(pretrained_weights) . . tokenized1 = tokenizer.encode(&quot;yes they are right&quot;, add_special_tokens=False) tokenized2 = tokenizer.encode(&quot;go to your right&quot;, add_special_tokens=False) print(tokenized1, tokenized2) # you can also get the full sentence using the token_ids print(tokenizer.decode(tokenized1)) print(tokenizer.decode(tokenized2)) . [2748, 2027, 2024, 2157] [2175, 2000, 2115, 2157] yes they are right go to your right . input_ids = torch.tensor([tokenized1, tokenized2]) model.eval() with torch.no_grad(): outputs = model(input_ids) last_hidden_states = outputs[0] . right1_bert = (last_hidden_states[0][-1]).numpy() right2_bert = (last_hidden_states[1][-1]).numpy() . print(right1_bert.shape, right2_bert.shape) . (768,) (768,) . plot_1d_heatmap(right1_bert, name=&quot;BERT vec for right in &quot;yes they are right &quot;&quot;) plot_1d_heatmap(right2_bert, name=&quot;BERT vec for right in &quot;go to your right &quot;&quot;) . print(&quot;Simialrity between &quot;right &quot; from sentence 1 &amp; 2 using BERT: t{}&quot;. format(similarity_between_docs(right1_bert, right2_bert, is_1d=True))) print(&quot;Simialrity between &quot;right &quot; from sentence 1 only using BERT: t{}&quot;. format(similarity_between_docs(right1_bert, right1_bert, is_1d=True))) print(&quot;Simialrity between &quot;right &quot; from sentence 2 only using BERT: t{}&quot;. format(similarity_between_docs(right2_bert, right2_bert, is_1d=True))) . Simialrity between &#34;right&#34; from sentence 1 &amp; 2 using BERT: 0.605478048324585 Simialrity between &#34;right&#34; from sentence 1 only using BERT: 0.9999997615814209 Simialrity between &#34;right&#34; from sentence 2 only using BERT: 0.9999997615814209 . Document Ranking . Let&#39;s use these embeddings to retrieve the documents based on a query . Load the data . !mkdir data !cd data &amp;&amp; curl -O https://raw.githubusercontent.com/ashishu007/Word-Embeddings/master/data/abstracts.csv !cd data &amp;&amp; curl -O https://raw.githubusercontent.com/ashishu007/Word-Embeddings/master/data/train.tsv . % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 778k 100 778k 0 0 1569k 0 --:--:-- --:--:-- --:--:-- 1569k % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 703k 100 703k 0 0 3431k 0 --:--:-- --:--:-- --:--:-- 3431k . dfa = pd.read_csv(&#39;./data/abstracts.csv&#39;) print(dfa.shape) dfa = dfa[:50] dfa.head(5) . (792, 2) . title content . 0 Understanding Human Language: Can NLP and Deep... | There is a lot of overlap between the core pro... | . 1 Big Data in Climate: Opportunities and Challen... | This talk will present an overview of research... | . 2 A Sequential Decision Formulation of the Inter... | The Interface Card model is a promising new th... | . 3 Audio Features Affected by Music Expressivenes... | Within a Music Information Retrieval perspecti... | . 4 Automatic Identification and Contextual Reform... | Web search functionality is increasingly integ... | . Some utility functions . import nltk nltk.download(&#39;punkt&#39;) from nltk.stem import PorterStemmer from nltk.corpus import stopwords from sentence_transformers import SentenceTransformer . [nltk_data] Downloading package punkt to /root/nltk_data... [nltk_data] Unzipping tokenizers/punkt.zip. . torch.__version__ . &#39;1.6.0+cu101&#39; . def gen_w2v_embs(row): # row = row[&#39;text&#39;] # print(row) tokens = nltk.word_tokenize(row) token_words = [w for w in tokens if w.isalpha()] # print(token_words) stemming = PorterStemmer() tokens_stemmed = [stemming.stem(word) for word in token_words] # print(tokens_stemmed) # stops = set(stopwords.words(&quot;english&quot;)) stops = [&quot;a&quot;, &quot;an&quot;, &quot;the&quot;] meaningful_words = [w for w in tokens_stemmed if not w in stops] # print(meaningful_words) vecs = [] for w in meaningful_words: w_vec = w2v_vectors.query(w) vecs.append(w_vec) vec_arr = np.array(vecs) vec_final = np.mean(vec_arr, axis=0, dtype=&quot;float32&quot;) return vec_final . # Here we use a different library called `sentence_transformers` beacuse this # library is easier than `transformers` for sentence embeddings def gen_bert_embs(col): bert_model = SentenceTransformer(&#39;distilbert-base-nli-mean-tokens&#39;) bert_embs = bert_model.encode(col) return bert_embs . Embedding the documents with different algorithms . w2v_abs = dfa[&quot;content&quot;].apply(gen_w2v_embs) w2v_abs = (torch.tensor(w2v_abs)).numpy() . elmo_abs = dfa[&quot;content&quot;].apply((lambda x: elmo_vecs.query(x))) elmo_abs = (torch.tensor(elmo_abs)).numpy() . /usr/local/lib/python3.6/dist-packages/pymagnitude/third_party/allennlp/nn/util.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor). index_range = sequence_lengths.new_tensor(torch.arange(0, len(sequence_lengths))) . bert_abs = gen_bert_embs(dfa[&quot;content&quot;]) . 100%|██████████| 245M/245M [00:10&lt;00:00, 22.3MB/s] . w2v_abs.shape, elmo_abs.shape, bert_abs.shape . ((50, 300), (50, 768), (50, 768)) . type(w2v_abs), type(elmo_abs), type(bert_abs) . (numpy.ndarray, numpy.ndarray, numpy.ndarray) . Perform natural language queries . def gen_query_emb(q, emb=&quot;w2v&quot;): if emb == &quot;w2v&quot;: query_emb = gen_w2v_embs(q) elif emb == &quot;elmo&quot;: query_emb = elmo_vecs.query(q) elif emb == &quot;bert&quot;: query_bert = gen_bert_embs(q) query_emb = query_bert.reshape(-1) return query_emb . q1 = gen_query_emb(&quot;documents that discuss learning methods&quot;, emb=&quot;elmo&quot;) q2 = gen_query_emb(&quot;documents that discuss learning methods&quot;, emb=&quot;bert&quot;) q3 = gen_query_emb(&quot;documents that discuss learning methods&quot;, emb=&quot;w2v&quot;) . /usr/local/lib/python3.6/dist-packages/pymagnitude/third_party/allennlp/nn/util.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor). index_range = sequence_lengths.new_tensor(torch.arange(0, len(sequence_lengths))) . q1.shape, q2.shape, q3.shape . ((768,), (768,), (300,)) . Get the similarity between query and documents . def get_doc_similarity(q, docs): sims = {} for i, doc in enumerate(docs): sim_score = similarity_between_docs(q, doc, is_1d=True) sims[i] = sim_score sims_sorted = {k: v for k, v in sorted(sims.items(), key=lambda item: item[1], reverse=True)} return sims_sorted . s = get_doc_similarity(q2, bert_abs) # s . ss = list(s.keys())[:10] ss . [17, 0, 34, 23, 8, 13, 42, 15, 11, 2] . dfa[&quot;content&quot;][17] . &#39;Doc2Sent2Vec is an unsupervised approach to learn low-dimensional feature vector (or embedding) for a document. This embedding captures the semantics of the document and can be fed as input to machine learning algorithms to solve a myriad number of applications in the field of data mining and information retrieval. Some of these applications include document classification, retrieval, and ranking.&#39; . Exercises . &quot;&quot;&quot; Try ranking same documents using another natural language query and ranking algorithm &quot;&quot;&quot; # Your code here . &#39; nTry ranking documents using another natural language query and ranking algorithm n&#39; . &quot;&quot;&quot; Print top 3 documents ranked by each query and algorithm. Compare their results &quot;&quot;&quot; # Your code here . &#39; nPrint top 3 documents ranked by each query and algorithm. Compare their results n&#39; . &quot;&quot;&quot; Plot the heatmap for a sentence using BERT and ELMo &quot;&quot;&quot; # Your code here . &#39; nPlot heatmap for a sentence using BERT and ELMo n&#39; . &quot;&quot;&quot; Plot the heatmap for same sentence using Word2Vec &quot;&quot;&quot; # Your code here . Text Classification . Let&#39;s apply different embeddings for a simple Text Classification problem . Load the data . df = pd.read_csv(&#39;./data/train.tsv&#39;, delimiter=&#39; t&#39;, names=[&quot;text&quot;, &quot;label&quot;]) print(df.shape) df = df[:200] df.head(5) . (6920, 2) . text label . 0 a stirring , funny and finally transporting re... | 1 | . 1 apparently reassembled from the cutting room f... | 0 | . 2 they presume their audience wo n&#39;t sit still f... | 0 | . 3 this is a visually stunning rumination on love... | 1 | . 4 jonathan parker &#39;s bartleby should have been t... | 1 | . Embedding the data using Word2Vec . w2vs = df[&quot;text&quot;].apply(gen_w2v_embs) . w2v_embs = (torch.tensor(w2vs)).numpy() w2v_embs.shape . (200, 300) . Embedding the data using ELMo . elmos = df[&quot;text&quot;].apply((lambda x: elmo_vecs.query(x))) . /usr/local/lib/python3.6/dist-packages/pymagnitude/third_party/allennlp/nn/util.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor). index_range = sequence_lengths.new_tensor(torch.arange(0, len(sequence_lengths))) . elmo_embs = (torch.tensor(elmos)).numpy() elmo_embs.shape . (200, 768) . Embedding the data using BERT . bert_embs = gen_bert_embs(df[&quot;text&quot;]) . bert_embs.shape . (200, 768) . Prepare the features and labels for train and test . labels = df[&quot;label&quot;] . from sklearn.model_selection import train_test_split X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(w2v_embs, labels, test_size=0.33, random_state=42, stratify=labels) X_train_elmo, X_test_elmo, y_train_elmo, y_test_elmo = train_test_split(elmo_embs, labels, test_size=0.33, random_state=42, stratify=labels) X_train_bert, X_test_bert, y_train_bert, y_test_bert = train_test_split(bert_embs, labels, test_size=0.33, random_state=42, stratify=labels) . from sklearn.linear_model import LogisticRegression lr_clf_w2v = LogisticRegression() lr_clf_w2v.fit(X_train_w2v, y_train_w2v) lr_clf_elmo = LogisticRegression() lr_clf_elmo.fit(X_train_elmo, y_train_elmo) lr_clf_bert = LogisticRegression() lr_clf_bert.fit(X_train_bert, y_train_bert) . /usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) . LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=None, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0, warm_start=False) . y_pred_w2v = lr_clf_w2v.predict(X_test_w2v) y_pred_elmo = lr_clf_elmo.predict(X_test_elmo) y_pred_bert = lr_clf_bert.predict(X_test_bert) . from sklearn.metrics import accuracy_score, f1_score print(&quot;Word2Vec tAccuracy: {} tMacro F1: {}&quot;.format(accuracy_score(y_pred_w2v, y_test_w2v), f1_score(y_pred_w2v, y_test_w2v, average=&quot;macro&quot;))) print(&quot;ELMo tAccuracy: {} tMacro F1: {}&quot;.format(accuracy_score(y_pred_elmo, y_test_elmo), f1_score(y_pred_elmo, y_test_elmo, average=&quot;macro&quot;))) print(&quot;BERT tAccuracy: {} tMacro F1: {}&quot;.format(accuracy_score(y_pred_bert, y_test_bert), f1_score(y_pred_bert, y_test_bert, average=&quot;macro&quot;))) . Word2Vec Accuracy: 0.5757575757575758 Macro F1: 0.396078431372549 ELMo Accuracy: 0.5454545454545454 Macro F1: 0.5170731707317072 BERT Accuracy: 0.8333333333333334 Macro F1: 0.832371276841376 .",
            "url": "https://ashishu007.github.io/nlp-blog/jupyter/nlp/2020/11/22/Semantic_Representation.html",
            "relUrl": "/jupyter/nlp/2020/11/22/Semantic_Representation.html",
            "date": " • Nov 22, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ashishu007.github.io/nlp-blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ashishu007.github.io/nlp-blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "To know more about me, check out my website. .",
          "url": "https://ashishu007.github.io/nlp-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ashishu007.github.io/nlp-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}