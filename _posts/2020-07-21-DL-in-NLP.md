---
toc: true
layout: post
comments: true
description: Quick overview of Deep Learning in NLP.
categories: [Blog, NLP, Transfer Learning, Deep Learning, Transformers, LSTMs, RNNs]
title: Deep Learning in Natural Language Processing
---

# Deep Learning in Natural Language Processing

I read many papers about Deep Learning (DL) in Natural Language Processing (NLP) in the starting months of my PhD. This blog is a summary of that reading, where I try to sequentially discuss the development of DL methods applied to various NLP tasks.

<!--more-->

A little heads-up about the blog. This blog is **not** an in-depth analysis of DL methods applied to NLP tasks, instead, I have tried to provide a **quick overview** of the main development of DL methods for NLP. The goal of this post is to provide the reader with a direction for starting with DL in NLP. I'll try to showcase the field's evolution in a previous couple of years. Hopefully, by the end of this blog, if you are a newcomer to the field, you'll at least have the understanding of technical terms of DL in NLP.

So, without further ado, ***let's start***.
<!-- 
**Table of Contents**

- [Artificial Intelligence - Looking Back](#artificial-intelligence---looking-back)
- [Word Embeddings](#word-embeddings)
- [Sequence Learning](#sequence-learning)
  * [Vanilla seq2seq Model](#vanilla-seq2seq-model)
  * [seq2seq with Attention](#seq2seq-with-attention)
- [Learning Methods](#learning-methods)
  * [Recurrent Neural Networks](#recurrent-neural-networks)
  * [Long Short-Term Memory Networks](#long-short-term-memory-networks)
  * [Convolutional Neural Networks](#convolutional-neural-networks)
  * [Transformers](#transformers)
- [Language Models](#language-models)
- [Transfer Learning](#transfer-learning)
  * [Embeddings Learned from Language Models](#embeddings-learned-from-language-models)
  * [Universal Language Model Fine-Tuning](#universal-language-model-fine-tuning)
  * [Generative Pre-Training](#generative-pre-training)
  * [Bidirectional Encoder Representation from Transformers](#bidirectional-encoder-representation-from-transformers)
- [Further Steps](#further-steps)
- [Acknowledgement](#acknowledgement)

- [References](#references)

<small><i><a href='http://ecotrust-canada.github.io/markdown-toc/'>Table of contents generated with markdown-toc</a></i></small> -->

## Artificial Intelligence - Looking Back

First, a quick recap of the developments in Artificial Intelligence (AI) research. In the early days of AI, before the 90s, experts used to design task specific **rules** for different real-world problems. However, they would often fail when unseen or unexpected data/situation arrived. 

In the last 20 years, around the late 90s or so, statistical approaches to these problems started gaining traction. Now, instead of writing rules, human effort was involved in extracting different **features** that would tell a mathematical model to learn the relation between input and output space. Statistical models are capable of learning the rules themselves, by looking at the labelled examples. These approaches can be considered as the early days of machine learning. However, they still require domain expertise to engineer domain specific features.

Around 2010, when a huge amount of data and powerful computers started coming out, a subset of machine learning, called deep learning, using neural networks became a model of choice for learning from data. These models are capable of learning the multi-level hierarchy of features and thus do not require any explicit need for feature engineering. Human energy is now focused on determining the most suitable **architecture** and training setting for each task.

![Sesame Street](https://ashishu007.github.io/assets/nlp-dl/ss.png){:width="850px" style="display:block;margin-left:auto;margin-right:auto;"}

<div style="text-align: center;"><b>Sesame Street & Co.: Popular naming convention nowadays in NLP <a href="http://jalammar.github.io/illustrated-bert/">(source)</a></b></div>

From next sections, I'll talk about NLP.

<!-- I'll also **not** discuss any **specific NLP task**, because the aim of this blog is to showcase that how the field has evolved in the previous coupe of years. -->

<!-- I try to provide an higher level overview of main concepts of DL & NLP, and then, point towards other resources that I used to get the deeper understanding of that concept.  -->

<!-- - [Word Embeddings](#heading)
- [Language Models](#heading-1)
- [Recurrent Neural Networks](#heading-2)
- [Long-Short Term Memory Networks](#heading-3)
- [Convolutional Neural Networks](#heading-4)
- [Sequence-to-Sequence Models](#heading-5)
    - [Attention](#sub-heading)
- [Contextual Word Embeddings](#heading-6)
    - [ELMo](#sub-heading-1)
- [Transfer Learning](#heading-7)
    - [ULMFiT](#sub-heading-2)
- [Transformers](#heading-8)
    - [GPT](#sub-heading-3)
    - [BERT](#sub-heading-4) -->

<!-- With this, let's start the discussion from next section now. -->

<!-- The main research papers are also cited throughut the blog, so, it'll be easy for the reader to follow-up that paper. -->

<!-- **The reason behind this blog** - when I started my PhD, I was very confused by the broadness of NLP. I knew I wanted to do something in NLP and DL, but wasn't sure what. I started reading papers, blogs, course materials related to NLP (not just DL but other knowledge based methods as well). For my Postgraduate Certification module, I had to submit a research plan just after four months of the start of my PhD. The reviews that I got were focused on mainly one thing - the literature review was not critical and felt like a tutorial. And that was absolutely right, because, at that point, even though I had read alot about NLP, I had no idea that where to apply those things. So, I wasn't able to critique the works properly. But then I thought why not to share the literature review as a tutorial only, this can benifit many new-comers of NLP. That's how I wrote this blog. -->

<!-- So, that's why I wrote this blog. This is a significant part of my literature review that I submitted to the university. Please have a read and do comment your opinion on the quality of my writing and this blog itself.  -->

<!-- Initially, Deep Learning (DL) outperformed a lot of state-of-the-art (SOTA) algorithms in vision tasks and established itself as the benchmark solutions [[7]](#myfootnote7) [[17]](#myfootnote17). During the past few years with the advancement in computing powers and the availability of large datasets, the **Deep Learning Tsunami** [[12]](#myfootnote12) has taken over NLP as well.  -->

<!-- ### Introduction -->
<!-- Representation Learning or in common words Deep Learning (DL) can be considered as a subset of machine learning but has gained attention as a different field due to its success in various vision, text and speech tasks. The difference about DL that makes it perform better on various complex problems is that it doesn't require domain expertise to identify the features to represent a problem or case from previous experience. Instead it automatically learns the features during training process using a large set of training data on various iterations.  -->

## Word Embeddings
The first thing to start should be the representation of textual data in numerical format. Word Embeddings are used to represent words in a multi-dimensional vector form. A word <span class="math"><b>w<sub>i</sub></b></span> in vocabulary **V** is represented in the form of a vector of **n** dimensions. These vectors are generated by unsupervised training on a large corpus of words to gain the semantic similarities between the words. Algorithms like word2vec [[1]](myfootnote1) and GloVe [[2]](myfootnote2) are used to train these word embeddings. These word embeddings are generally pre-trained and made publicly available to be directly used in the deep learning models.

<!-- ![Figure 1: LSTM](/assets/nlp-dl/lstm.png){:width="500px" style="display:block;margin-left:auto;margin-right:auto;"} -->

![Word Embeddings](https://ashishu007.github.io/assets/nlp-dl/glove_vecs.png){:width="500px" style="display:block;margin-left:auto;margin-right:auto;"}
<div style="text-align: center;"><b>2-D visualisation of GloVe vectors <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/slides/cs224n-2019-lecture02-wordvecs2.pdf">(source)</a></b></div>

These distributed representations of words give a certain amount of semantic understanding of the words in a high dimensional vector space. For example, the distance between words **_king_** and **_queen_** will be similar to the distance between **_boy_** and **_girl_**. In this contrast, the below equation fits perfectly.

<!-- % A visualization of these word embeddings in low dimension is shown in \cref{fig:glove_ex} \footnote{taken from \url{http://web.stanford.edu/class/cs224n/}}. -->

<div style="text-align: center;">
<span class="math"><b>boy - girl + king = queen</b></span>
</div>

These representations have a drawback that they fail to take the context of a word into account. For example, the word **_'Scotland'_** will have a different meaning in the sentence **_'Scotland is one of the best places to live on earth'_** than in the sentence **_'Royal Bank of Scotland is one of the top banking firms in the UK'_**. The GloVe or word2vec word embeddings will fail to differentiate the two meanings of **_Scotland_** and will assign the same vector in both the cases. These drawbacks are tackled by a new concept of contextual word embeddings discussed in a [section below](#embeddings-learned-from-language-models).


- **Other Resources**
    - [This lecture from CS224u](http://web.stanford.edu/class/cs224u/materials/cs224u-2020-vsm-handout.pdf).
    - [Word2Vec Paper](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf).
    - [GloVe Paper](https://nlp.stanford.edu/pubs/glove.pdf).

## Sequence Learning

Most NLP tasks require sequential output instead of a single output label, unlike classification or regression, for example, Machine Translation, Question-Answering, or Named-Entity Recognition. These systems take a sequence of input and process it to produce yet another sequence for output. The goal is to take a sequence <span class="math"><b>x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>n</sub></b></span> as input and map it to another sequence <span class="math"><b>y<sub>1</sub>, y<sub>2</sub>, ..., y<sub>m</sub></b></span> as output.

### Vanilla seq2seq Model
The architecture used to deal with this kind of problems is known as **Sequence-to-Sequence** model or in common terms, **seq2seq** model. It is a combination of encoders and decoders which works in a sequential manner, where, an encoder is a neural network that generates a context vector from the input sequence, and decoder, another neural network taking context vector as input, generates the output sequence.

The encoder takes an input **X** and maps it to fixed-size context vector **Z** using the formula given below:

<div style="text-align: center;">
<span class="math"><b>Z = &sigma;(Wx + b)</b></span>
</div>

<!-- \begin{equation}
    \label{eq:enc}
        Z = \sigma(WX + b)
\end{equation} -->

where <span class="math"><b>&sigma;</b></span> is an **activation function**. A decoder then maps the context vector **Z** to a new form of input <span class="math"><b>X'</b></span> as shown in the equation below:

<div style="text-align: center;">
<span class="math"><b>X' = &sigma;'(W'Z + b')</b></span>
</div>

<!-- \begin{equation}
    \label{eq:dec}
        X^{\prime} = \sigma^{\prime}(W^{\prime}Z + b^{\prime})
\end{equation} -->

where <span class="math"><b>&sigma;'</b></span> is another **activation function**. The loss is calculated as the **squared error** between original and reconstructed input as shown in equation below:

<div style="text-align: center;">
<span class="math"><b>L = || X - X' ||<sup>2</sup></b></span>
</div>

<!-- \begin{equation}
    \label{eq:seq2seq_loss}
        L = ||X-X^{\prime}||^{2}
\end{equation} -->

An illustrated diagram of seq2seq model performing machine translation is shown in the figure below:

![Encoder](https://ashishu007.github.io/assets/nlp-dl/encoder.png){:width="250px" style="display:block;margin-left:auto;margin-right:auto;"}
<div style="text-align: center;"><b>Encoder</b></div>

![Decoder](https://ashishu007.github.io/assets/nlp-dl/decoder.png){:width="250px" style="display:block;margin-left:auto;margin-right:auto;"}
<div style="text-align: center;"><b>Decoder</b></div>

<div style="text-align: center;"><b>An Encoder-Decoder seq2seq model <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/readings/cs224n-2019-notes06-NMT_seq2seq_attention.pdf">(source)</a></b></div>

<!-- \begin{figure}
     \centering
     \begin{subfigure}[b]{0.25\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/encoder.png}
         \caption{Encoder}
         \label{fig:encoder}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.25\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/decoder.png}
         \caption{Decoder}
         \label{fig:decoder}
     \end{subfigure}
        \caption{Encoder-Decoder architecture using LSTM for seq2seq model}
        \label{fig:enc_dec}
\end{figure} -->


### seq2seq with Attention 

A problem with general encoder-decoder seq2seq model is that they give equal importance to all parts of the input sequence. Also, the input sequence is compressed into a single context vector which creates the bottleneck problem, where a piece of long information is tried to be compressed into one small representation.

A solution to this problem was proposed in the work [[3]](#myfootnote3) introducing a new mechanism called **Attention**. Attention aligns the output at each decoding step to the whole input sequence in order to learn the most important part of the input aligning with the current step output.

Let's say we have calculated the encoding hidden states <span class="math"><b>h<sub>1</sub>, h<sub>2</sub>, ..., h<sub>n</sub></b></span> for the input sequence <span class="math"><b>x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>n</sub></b></span> during the calculation of context vector **Z**. For a decoder hidden state <span class="math"><b>s<sub>t</sub></b></span> on timestep <span class="math"><b>t</b></span>, we get attention score <span class="math"><b>e<sup>t</sup></b></span> as follows:

<div style="text-align: center;">
<a href="https://www.codecogs.com/eqnedit.php?latex=e^{t}&space;=&space;[s^{T}_{t}h_{1},&space;\cdots,&space;s^{T}_{t}h_{n}]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?e^{t}&space;=&space;[s^{T}_{t}h_{1},&space;\cdots,&space;s^{T}_{t}h_{n}]" title="e^{t} = [s^{T}_{t}h_{1}, \cdots, s^{T}_{t}h_{n}]" /></a>
<!-- <span class="math"><b>e<sup>t</sup> = [s<sub>t</sub><sup>T</sup>h<sub>1</sub>, ..., s<sub>t</sub><sup>T</sup>h<sub>n</sub>]</b></span> -->
</div>

<!-- \begin{equation}
    \label{eq:att_score}
        e^{t} = [s^{T}_{t}h_{1}, \cdots, s^{T}_{t}h_{n}]
\end{equation} -->

<!-- e^{t} = [s^{T}_{t}h_{1}, \cdots, s^{T}_{t}h_{n}] -->

We take the softmax of these scores to get the attention distribution at timestep **t**.

<div style="text-align: center;">
<a href="https://www.codecogs.com/eqnedit.php?latex=\alpha^{t}&space;=&space;softmax(e^{t})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\alpha^{t}&space;=&space;softmax(e^{t})" title="\alpha^{t} = softmax(e^{t})" /></a>
<!-- <span class="math"><b>&prop;<sup>t</sup> = softmax(e<sup>t</sup>)</b></span> -->
</div>

<!-- \begin{equation}
    \label{eq:att_dist}
        \alpha^{t} = softmax(e^{t})
\end{equation} -->

The attention output <span class="math"><b>a<sub>t</sub></b></span> is then calculated as the weighted sum of encoder hidden state using <span class="math"><b>&prop;<sup>t</sup></b></span>:

<div style="text-align: center;">
<!-- <span class="math"><b>a<sub>t</sub> = &prop;<sup>t</sup><sub>i</sub>h<sub>i</sub></b></span> -->
<a href="https://www.codecogs.com/eqnedit.php?latex=a_{t}&space;=&space;\sum_{i=1}^{n}&space;\alpha_{i}^{t}h_{i}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?a_{t}&space;=&space;\sum_{i=1}^{n}&space;\alpha_{i}^{t}h_{i}" title="a_{t} = \sum_{i=1}^{n} \alpha_{i}^{t}h_{i}" /></a>
</div>

<!-- \begin{equation}
    \label{eq:att_out}
        a_{t} = \sum_{i=1}^{n} \alpha_{i}^{t}h_{i}
\end{equation} -->

Finally, we concatenate the attention output <span class="math"><b>a<sup>t</sup></b></span> with decoder hidden state <span class="math"><b>s<sup>t</sup></b></span> and proceed to calculate the negative log loss same as the non-attention decoder model.

An attentive machine translation model is shown in the figure below:

![Attention](https://ashishu007.github.io/assets/nlp-dl/attn.jpg){:width="750px" style="display:block;margin-left:auto;margin-right:auto;"}
<div style="text-align: center;"><b>Attention model <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/slides/cs224n-2019-lecture08-nmt.pdf">(source)</a></b></div>

- **Other Resources**
    - Have a look at [this lecture from CS224n](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/slides/cs224n-2019-lecture08-nmt.pdf).
    - Read the [seq2seq paper](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf).
    - Read this [attention paper](https://mila.quebec/wp-content/uploads/2019/08/1409.0473.pdf).


## Learning Methods

Now let's talk about the learning algorithms that are capable of learning the mapping between input and output representations. Here, we'll take a look at some of the backbone neural networks (DL architectures) used in various NLP tasks.

### Recurrent Neural Networks
Text is a sequential form of data. To process text and extract information from it, we need a model capable of processing the sequential data. **Recurrent Neural Networks (RNNs)** [[4]](#myfootnote4) are the most elementary deep learning architectures that are able to learn from sequential data. RNNs are wide in nature as they unroll through time. These networks will have a _'memory'_ component which can store the information about the previous state. They share the same set of weights throughout the layers, however, will receive a new input at every layer or time-step. 

The output to every time-step is dependent on the input taken at the current time-step <span class="math"><b>t<sub>i</sub></b></span> as well as the information gained from previous time-step <span class="math"><b>t<sub>i-1</sub></b></span>. Specifically, an RNN will maintain a hidden state <span class="math"><b>h<sub>t</sub></b></span> at every step which is referred as the memory of network. An illustrated diagram of unrolled RNN is shown in figure below 

![RNN](https://ashishu007.github.io/assets/nlp-dl/rnn.png){:width="500px" style="display:block;margin-left:auto;margin-right:auto;"}
<div style="text-align: center;"><b>A simple Recurrent Neural Network <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">(source)</a></b></div>

The operations performed in RNN at every time step is given in the equations below:

<div style="text-align: center;">
<span class="math"><b>h<sub>t</sub> = &sigma;<sub>h</sub> ( W<sub>e</sub>x<sub>t</sub> + W<sub>h</sub>h<sub>t-1</sub> + b<sub>h</sub> )</b></span>
</div>

<div style="text-align: center;">
<span class="math"><b>y<sub>t</sub> = &sigma;<sub>y</sub> ( W<sub>y</sub>h<sub>t</sub> + b<sub>y</sub> )</b></span>
</div>

<!-- \begin{align}\label{eq:rnn}
    \begin{gathered}
        h_{t} = \sigma_{h}(W_{e}x_{t} + W_{h}h_{t-1} + b_{h}) \\
        y_{t} = \sigma_{y}(W_{y}h_{t} + b_{y})
    \end{gathered}
\end{align} -->

Here <span class="math"><b>&sigma;<sub>y</sub></b></span> and <span class="math"><b>&sigma;<sub>h</sub></b></span> are the activation functions. <span class="math"><b>W<sub>h</sub></b></span> is the weight matrix to apply transformation on previous hidden state <span class="math"><b>h<sub>t-1</sub>, W<sub>e</sub></b></span> is the weight matrix to apply transformation on the input <span class="math"><b>x<sub>t</sub></b></span> received over time **t**. Combining these with the bias <span class="math"><b>b<sub>h</sub></b></span> yields hidden state <span class="math"><b>h<sub>t</sub></b></span> for time **t**. Applying activation on the <span class="math"><b>h<sub>t</sub></b></span> with <span class="math"><b>W<sub>y</sub></b></span> gives the output <span class="math"><b>y<sub>t</sub></b></span> for every time-step **t**.

- **Other Resources**
    - Have a look at [this lecture from CS224n](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/slides/cs224n-2019-lecture06-rnnlm.pdf).

### Long Short-Term Memory Networks
Although, in theory the RNNs are designed to handle the sequence input but in practice they lack in storing the long term dependencies because of the problem of exploding and vanishing gradients [[5]](#myfootnote5). Long Short-Term Memory Networks [[6]](#myfootnote6) are the advanced version of RNNs with a slight modification of being capable of deciding what to **_'remember'_** and what to ***forget*** from the input sequence with the help of a series of gates. LSTMs have a number of gates: an output gate <span class="math"><b>o<sub>t</sub></b></span>; an input gate <span class="math"><b>i<sub>t</sub></b></span>; a forget gate <span class="math"><b>f<sub>t</sub></b></span> - all of which are the functions of previous hidden state <span class="math"><b>h<sub>t</sub></b></span> and current input <span class="math"><b>x<sub>t</sub></b></span>. These gates interact with the previous cell state <span class="math"><b>c<sub>t-1</sub></b></span>, the current input <span class="math"><b>x<sub>t</sub></b></span>, and the current cell state <span class="math"><b>c<sub>t</sub></b></span> and enable the model to selectively retain or information from the sequence. The full version of LSTM is given in the equation below.

<!-- \begin{equation}\label{eq:lstm}
    \begin{gathered}    
        f_{t} = \sigma_{g} (W_{f}x_{t} + U_{f}h_{t-1} + b_{f}) \\
        i_{t} = \sigma_{g} (W_{i}x_{t} + U_{i}h_{t-1} + b_{i}) \\
        o_{t} = \sigma_{g} (W_{o}x_{t} + U_{o}h_{t-1} + b_{o}) \\
        \Tilde{c_{t}} = \sigma_{c} (W_{c}x_{t} + U_{c}h_{t-1} + b_c)\\
        c_{t} = f_{t} \circ c_{t-1} + i_{t} \circ \Tilde{c_{t}}\\
        h_{t} = o_{t} \circ \sigma_{h}(c_{t})
    \end{gathered}
\end{equation} -->

<div style="text-align: center;">
<span class="math"><b>f<sub>t</sub> = &sigma;<sub>g</sub> ( W<sub>f</sub>x<sub>t</sub> + U<sub>f</sub>h<sub>t-1</sub> + b<sub>f</sub> )</b></span>
</div>

<div style="text-align: center;">
<span class="math"><b>i<sub>t</sub> = &sigma;<sub>g</sub> ( W<sub>i</sub>x<sub>t</sub> + U<sub>i</sub>h<sub>t-1</sub> + b<sub>i</sub> )</b></span>
</div>

<div style="text-align: center;">
<span class="math"><b>o<sub>t</sub> = &sigma;<sub>g</sub> ( W<sub>o</sub>x<sub>t</sub> + U<sub>o</sub>h<sub>t-1</sub> + b<sub>o</sub> )</b></span>
</div>

<div style="text-align: center;">
<span class="math"><b>&#265;<sub>t</sub> = &sigma;<sub>c</sub> ( W<sub>c</sub>x<sub>t</sub> + U<sub>c</sub>h<sub>t-1</sub> + b<sub>c</sub> )</b></span>
</div>

<div style="text-align: center;">
<span class="math"><b>c<sub>t</sub> = f<sub>t</sub> &#x25CB; c<sub>t-1</sub> + i<sub>t</sub> &#x25CB; &#265;<sub>t</sub></b></span>
</div>

<div style="text-align: center;">
<span class="math"><b>h<sub>t</sub> = o<sub>t</sub> &#x25CB; &sigma;<sub>h</sub>(c<sub>t</sub>)</b></span>
</div>


where <span class="math"><b>&sigma;<sub>g</sub></b></span> is the **sigmoid activation function**, <span class="math"><b>&sigma;<sub>c</sub></b></span> and <span class="math"><b>&sigma;<sub>h</sub></b></span> are the **tanh activation function**, and <span class="math"><b>&#x25CB;</b></span> is element-wise multiplication, also known as **Hadamard product**. An illustrated diagram of LSTM is shown below:

![LSTM](https://ashishu007.github.io/assets/nlp-dl/lstm.png){:width="500px" style="display:block;margin-left:auto;margin-right:auto;"}
<div style="text-align: center;"><b>A simple Long-Short Term Memory Network  <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">(source)</a></b></div>

<!-- \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{images/lstm.png}
    \caption{LSTM}
    \label{fig:lstm}
\end{figure} -->

LSTM layers can be stacked on each other to form multi-layer LSTM architecture. One of the most popular LSTM architecture is Bidirectional LSTM (BiLSTM), where two separate LSTMs are used to capture the sequential information in both forward and backwards directions. Gated Recurrent Units (GRUs) [[7]](#myfootnote7) are another advanced version of RNN which are common for handling the vanishing gradient problem, but I am not discussing them here as they are almost similar to LSTM except with no cell-state.

- **Other Resources**
    - Go through [this amazing blog from Christopher Olah](https://colah.github.io/posts/2015-08-Understanding-LSTMs/).


### Convolutional Neural Networks
Convolutional Neural Networks (CNNs) [[8]](#myfootnote8) have established themselves as state-of-the-art in various computer vision tasks. For the last couple of years, CNNs have been the benchmark for almost every vision task. Inspired from the popularity of CNN in vision, Yoon Kim proposed a CNN architecture for sentence classification which outperformed then benchmarks on various text classification datasets [[9]](#myfootnote9). 

<!-- \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{images/cnn_text.png}
    \caption{CNN for sentence classification}
    \label{fig:cnn}
\end{figure} -->

<!-- CNNs are capable of learning vectors for all possible sub-phrases in a sentence, not just grammatically correct one as done by RNNs.  -->
A CNN takes an input sentence of **n** words, where each word is represented using a vector of dimension **d**. The input <span class="math"><b>X<sub>1:n</sub></b></span> will be a **2D** matrix of shape <span class="math"><b>n &times; d</b></span>, where <span class="math"><b>x<sub>i</sub> &isin; &#x211D; <sup>d</sup> </b></span>. Input <span class="math"><b>X<sub>1:n</sub></b></span> can be represented as:

<div style="text-align: center;">
<span class="math"><b>X<sub>1:n</sub> = x<sub>1</sub> &oplus;, ..., &oplus; x<sub>n</sub></b></span>
</div>

<!-- \begin{equation}
    \label{eq:cnn_input}
    X_{1:n} = x_{1} \oplus x_{2} \oplus \cdots \oplus x_{n}
\end{equation} -->

where <span class="math"><b>&oplus;</b></span> is the concatenation operator. On the input layer, convolution filter <span class="math"><b>W &isin; &#x211D;<sup>hd</sup></b></span> is applied over window of **h** words to generate a new feature. So, a feature <span class="math"><b>c<sub>i</sub></b></span> is generated from the word window <span class="math"><b>x<sub>i:i+h-1</sub></b></span> with the following operation:

<div style="text-align: center;">
<span class="math"><b>x<sub>i</sub> = &sigma;(Wx<sub>i:i+h-1</sub> + b)</b></span>
</div>
<!-- \begin{equation}
    c_{i} = \sigma (Wx_{i:i+h-1} + b)
\end{equation} -->

where **W** is the weight matrix for the connections, <span class="math"><b>&sigma;</b></span> is the **activation function** and <span class="math"><b>b &isin; &#x211D;</b></span> is the bias. Now, this filter is applied to each possible window of words giving an feature map <span class="math"><b>C &isin; &#x211D;<sup>n-h+1</sup></b></span>.

<div style="text-align: center;">
<span class="math"><b>C = [c<sub>1</sub>, c<sub>2</sub>, ..., c<sub>n-h+1</sub>]</b></span>
</div>

<!-- \begin{equation}
    C = [c_{1}, c_{2}, \cdots, c_{n-h+1}]
\end{equation} -->

The entries in feature map **C** are sharing the parameter **W**, where each <span class="math"><b>c<sub>i</sub> &isin; C</b></span> is a result of calculation on small segment of the input. Then a **max-pooling** operation is applied on these feature maps to capture the most important part.

<div style="text-align: center;">
<span class="math"><b>&#264; = max(C)</b></span>
</div>

<!-- \begin{equation}
    \Tilde{C} = max(C)
\end{equation} -->

This parameter sharing helps the model to incorporate an inductive bias into the model, helping to become learn the location invariant local features. There are **k** number of filters applied to the input with different window sizes which are then concatenated to form a vector <span class="math"><b>K &isin; &#x211D;<sup>k</sup></b></span>. Which is then fed to the next hidden layer or output layer.

An illustrated diagram of a CNN architecture for text classification is shown in the figure below:

![CNN](https://ashishu007.github.io/assets/nlp-dl/cnn.png){:width="500px" style="display:block;margin-left:auto;margin-right:auto;"}
<div style="text-align: center;"><b>A simple CNN for Text Classification <a href="https://www.aclweb.org/anthology/D14-1181.pdf">(source)</a></b></div>

- **Other Resources**
    - Have a look at the [CNN for text classification paper](https://www.aclweb.org/anthology/D14-1181.pdf).
    - Also have a look at [this lecture from CS224n](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/slides/cs224n-2019-lecture11-convnets.pdf).


### Transformers

Most of the above models have recurrent behaviour, which can not be trained parallelly. This imposes a huge problem of time taken to train a model from scratch. In the work [[10]](#myfootnote10), authors proposed a new neural architecture called **Transformers** which uses a combination of **self-attention** and **feed-forward network** and doesn't require any recurrent or convolutional elements. Also, the self-attention module lets the model capture the long term dependencies in a sentence without having any effect of the sentence length.

This new model was a huge success gaining better performance on various sequential learning tasks. To name one, it improved the machine translation performance by **10 BLEU** on **WMT EN-FR and WMT EN-DE datasets**. It also reduced the training time by large margin benefiting from the non-recurrent behaviour.

The success of transformer architecture paved the way for the development of new models to solve the sequential tasks. It helped NLP researchers to utilize its non-recurrent nature in transfer learning where the transformer is used for general pre-training of a language model (LM). Then the LM fine-tuned on domain-specific dataset for downstream tasks. An encoder-decoder model using Transformer architecture is shown in the figure below:

![Transformers](https://ashishu007.github.io/assets/nlp-dl/transformer.png){:width="500px" style="display:block;margin-left:auto;margin-right:auto;"}
<div style="text-align: center;"><b>A Transformer <a href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf">(source)</a></b></div>

I cannot provide an in-depth overview of the Transformer but you can refer to resources below for a better understanding.

- **Other Resources**
    - Go through the main paper ["Attention is all you need"](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf).
    - Take a look at [this amazing blog](https://nlp.seas.harvard.edu/2018/04/03/attention.html).
    - Must read - [Jay Alammar's blog on Transformers](http://jalammar.github.io/illustrated-transformer/).



## Language Models
Another important concept we should be aware of in NLP is Language Models (LM), because transfer learning in NLP is applied using different versions of LMs only. A language model is a type of system that predicts the probability of possible next words for a given sequence of words as the input. 

![LM](https://ashishu007.github.io/assets/nlp-dl/lang_model.png){:width="500px" style="display:block;margin-left:auto;margin-right:auto;"}
<div style="text-align: center;"><b>A simple Language Model <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/slides/cs224n-2019-lecture06-rnnlm.pdf">(source)</a></b></div>

In general terms, for a given sequence of input <span class="math"><b>(x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>t</sub>)</b></span> the probability distribution of next term <span><b>(x<sub>t+1</sub>)</b></span> is computed from a vocabulary **V** of **k** words <span><b>(V = (w<sub>1</sub>, w<sub>1</sub>, ..., w<sub>k</sub>))</b></span> as given below:

<!-- $(x_{1}, x_{2}, \cdots, x_{t})$,  -->
<!-- $x_{t+1}$ -->
<!-- ($V = (w_{1}, w_{2}, \cdots, w_{k})$) -->

<!-- \begin{equation}
    \label{eq:lm_dist}
        P(x_{t+1}|x_{1}, \cdots, x_{t})
\end{equation} -->

<div style="text-align: center;">
<span class="math"><b>P(x<sub>t+1</sub>|x<sub>1</sub>, ..., x<sub>t</sub>)</b></span>
</div>

Earlier the language models were based on statistical approaches where they used to take a window of **n** words as a context from the sentence to predict the next word. This approach is also known as **n-gram** Language Model. It takes a simple approach of calculating the conditional probability of next word in the sentence given the window of **n** words as a context. These **n-gram** probabilities are calculated from counting them in some large corpus of text.

<div style="text-align: center;">
<span class="math"><b>P(x<sub>t+1</sub>|x<sub>1</sub>, ..., x<sub>t</sub>) = P(x<sub>t+1</sub>, x<sub>t</sub>, ..., x<sub>t-n+2</sub>) &frasl; P(x<sub>t</sub>, ..., x<sub>t-n+2</sub>)</b></span>
</div>

<!-- \begin{equation}
    \label{eq:lm_ngram}
        P(x_{t+1}|x_{1}, \cdots, x_{t}) = 
            \frac {P(x_{t+1}, x_{t}, \cdots, x_{t-n+2})} {P(x_{t}, \cdots, x_{t-n+2})}
\end{equation} -->

or in simpler terms:

<div style="text-align: center;">
<span class="math"><b>P(w<sub>3</sub>|w<sub>1</sub>, w<sub>2</sub>) = P(w<sub>1</sub>, w<sub>2</sub>, w<sub>3</sub>) &frasl; P(w<sub>1</sub>, w<sub>2</sub>)</b></span>
</div>

<!-- \begin{equation}
    \label{eq:lm_count_ngram}
        P(w_{3}|w_{1},w_{2}) = \frac{count(w_{1}, w_{2}, w_{3})}{count(w_{1}, w_{2})}
\end{equation} -->

This statistical approach has mainly two problems. First, sparsity - consider the above equation, what if <span class="math"><b>w<sub>1</sub>, w<sub>2</sub>, w<sub>3</sub></b></span> never occurred together, the probability of <span class="math"><b>w<sub>3</sub></b></span> will be **0**. Second, storage - as we increase the value of **n**, the count of all **n-grams** we see in the corpus increases as well and so does the need of memory to store them.

<!-- $w_{1}$, $w_{2}$ and $w_{3}$  -->

<!-- $w_{3}$  -->
<!-- Also, if <span class="math"><b>w<sub>1</sub> and w<sub>2</sub></b></span> $w_{1}$ and $w_{2}$ then there's no way to calculate the probability of $w_{3}$.  -->

The neural language models using RNNs and Transformers are capable of modelling all the words in a sentence without needing a window to predict the next word at each timestep. You can think of a simple RNN for neural LM, where the RNN takes input tokens one at a time and by processing them generates the hidden state of each timestep. At the final time step, the output of that RNN block will be taken as the softmax against the whole vocabulary, and the word with the highest probability will be the prediction of LM as next word in the sequence. 

- **Other Resources**
    - For a better and deeper understanding please refer to [this lecture from CS224n](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/slides/cs224n-2019-lecture06-rnnlm.pdf).

<!-- \subsubsection{Recurrent Neural Networks} \label{sec:rnn} -->

## Transfer Learning

NLP cracked transfer learning by applying a simple rule: first, learn the general nuances of text (grammar-rules or fill in the blanks) from a huge corpus of text using a language model; and then transfer that learning by fine-tuning the language model on a task-specific dataset. In the following sections, we'll discuss some of the advancements in using different versions of LMs for transfer learning. 
<!-- This was the ***The Imagenet Moment of NLP***. -->

### Embeddings Learned from Language Models

As discussed in the [above section](#word-embeddings), the word embeddings generated by algorithms like word2vec and GloVe lack the contextual awareness and fail to differentiate a word with different senses. A new version of words embeddings, known as, **contextual word embeddings** are capable of differentiating a word's sense based on its context. These new models use a Language Model (LM) to generate the contextualised representation of words in a sentence. These modified embeddings generated from the LMs can be used as the input to another neural network for some downstream tasks.

<!-- #### Embeddings Learned from Language Models -->

**ELMo** [[11]](#myfootnote11) is one such embedding algorithm that uses a **bidirectional language model** to capture the context of a word in a sentence from both sides (left to right and vice-versa). ELMo uses a **character-level CNN** to convert raw text into a word vector which is then fed into a bidirectional language model. The output of this BiLM is then sent to the next layer of BiLM to form a set of intermediate word vectors. The final output of ELMo is the weighted sum of raw vectors and the intermediate vectors formed from two layers of the BiLMs. The two language models used here are based on LSTM architectures. An illustration of ELMo is shown in the figure below:

![ELMo](https://ashishu007.github.io/assets/nlp-dl/elmo.png){:width="500px" style="display:block;margin-left:auto;margin-right:auto;"}
<div style="text-align: center;"><b>ELMo Model <a href="https://www.analyticsvidhya.com/blog/2019/03/learn-to-use-elmo-to-extract-features-from-text/">(source)</a></b></div>

ELMo achieved 9% error reduction on the SQuAD (question-answering) dataset compared to then SOTA, 16% on Ontonotes SRL dataset, 10% on Ontonotes coreference dataset and 4% on CoNLL 2003 dataset. The fact that these embeddings are contextual and have the knowledge of word senses, helps ELMo and other future models such as BERT and GPT perform so much better.

- **Other Resources**
    - Before ELMo, CoVe proposed a similar idea. Have a look at the [paper](http://papers.nips.cc/paper/7209-learned-in-translation-contextualized-word-vectors.pdf), if you fancy reading.
    - Read the [ELMo Paper](https://www.aclweb.org/anthology/N18-1202.pdf).
    - [Recommended notes for a quick read](https://arxiv.org/pdf/1902.06006.pdf).
    - Have a look at [this lecture from CS224u](http://web.stanford.edu/class/cs224u/materials/cs224u-2020-contextualreps-handout.pdf).
    - Also, go through [this amazing blog on Analytics Vidhya](https://www.analyticsvidhya.com/blog/2019/03/learn-to-use-elmo-to-extract-features-from-text/).


### Universal Language Model Fine-Tuning

Universal Language Model Fine-Tuning (ULMFiT) [[12]](#myfootnote12) can be considered as one of the pioneers of applying transfer learning on an NLP task (text classification). It does so in three main steps: first, training a general domain-independent language model on a large corpus of text; second, fine-tune the language model on task-specific target dataset; and third, again fine-tuning the fine-tuned language model as a classifier by adding a softmax activation on top with target dataset. An illustration of the three steps of ULMFiT is shown in the figure below:

![ULMFiT](https://ashishu007.github.io/assets/nlp-dl/ulmfit.jpg){:width="750px" style="display:block;margin-left:auto;margin-right:auto;"}
<div style="text-align: center;"><b>ULMFiT Model <a href="https://www.aclweb.org/anthology/P18-1031.pdf">(source)</a></b></div>

ULMFiT achieved better results for text classification on six different datasets ranging from topic classification to sentiment analysis. The fine-tuning approach employed by ULMFiT is also very interesting, where different learning rates are applied to different layers of the network. I would recommend reading the paper to get a better understanding of the updated version of backpropagation through time algorithm.

- **Other Resources**
    - **Definitely read** the [ULMFiT Paper](https://www.aclweb.org/anthology/P18-1031.pdf).
    - Have a look at [this amazing blog](https://humboldt-wi.github.io/blog/research/information_systems_1819/group4_ulmfit/).

### Generative Pre-Training

One of the earliest works in using Transformers for pre-training of language model and applying transfer learning was presented in Generative Pre-Training (GPT) [[13]](#myfootnote13). Following the idea from ELMo, authors proposed a **language model** using **transformer decoder** trained on a large corpus of text. The main difference of GPT from ELMo is that ELMo uses two independent LSTM language models to capture the forwards and backward context whereas, in case of GPT, it uses a uni-directional multi-layer transformer language model capable of capturing context due to its attentive nature.

**ELMo** takes a **feature-based approach** of generating feature vectors (or contextual representations of the sentences) for different tasks, whereas **GPT** takes a **fine-tuning based approach** where the same language model trained on huge corpus is fine-tuned on task-specific data for downstream tasks. An illustration of a GPT model used for pre-training is shown in the figure below:

![GPT](https://ashishu007.github.io/assets/nlp-dl/g.png){:width="500px" style="display:block;margin-left:auto;margin-right:auto;"}
<div style="text-align: center;"><b>GPT Model <a href="https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html#openai-gpt">(source)</a></b></div>

<!-- \begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{images/GPT.png}
    \caption{GPT architecture.}
    \label{fig:gpt}
\end{figure} -->

- **Other Resources**
    - Read the [GPT Paper](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf).
    - The updated version [GPT-2 Paper](https://openai.com/blog/better-language-models/).
    - A [blog post](https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html#openai-gpt) similar to this one.

### Bidirectional Encoder Representation from Transformers

Bidirectional Encoder Representation from Transformers (BERT) [[14]](#myfootnote14) is another example of the success of transfer learning in NLP. BERT is a bidirectional transformer language model trained on a large text corpus that can be fine-tuned on any domain-specific dataset for the downstream tasks such as text classification, or named entity recognition. BERT mainly differs from other models like GPT and ELMo because of the pre-training tasks used during the unsupervised training of language model. Its pre-training is based on two tasks: first, **Masked Language Model (MLM)**; and second, **prediction of next sentence** from the corpora.

For the first task of **Masked Language Model**, let’s say we have a sentence ***'Boris Johnson is the Prime Minister of UK'***. So instead of training for prediction of next word in the sentence as a general Language Model, BERT pre-training replaces 15% of the words with a **[MASK] token** and learns to predict the correct word at the position of **[MASK] token**. In the second task of **Next Sentence Prediction**, the model is trained to learn the relationship between sentences where, for a given sentence pair A & B, the model is asked to predict if the sentence B is the next sentence that comes after A.

BERT improved the fine-tuning based approach of GPT by using a bidirectional transformer and learning both left & right context at the same time. This gave a huge improvement over GPT's unidirectional approach especially for token-level tasks like Question Answering, where the answer depends on both left and right contexts. An illustrated diagram of BERT pre-training and fine-tuning is shown in the figure below:

![BERT 1](https://ashishu007.github.io/assets/nlp-dl/bert1.png){:width="500px" style="display:block;margin-left:auto;margin-right:auto;"}
<div style="text-align: center;"><b>BERT Model <a href="https://arxiv.org/pdf/1810.04805.pdf">(source)</a></b></div>

A visual comparison between BERT, GPT and ELMo architectures presented in the paper is shown below:

![BERT 2](https://ashishu007.github.io/assets/nlp-dl/bert2.png){:width="500px" style="display:block;margin-left:auto;margin-right:auto;"}
<div style="text-align: center;"><b>BERT, GPT and ELMo <a href="https://arxiv.org/pdf/1810.04805.pdf">(source)</a></b></div>

We can see, BERT uses the bi-directional transformer for processing the sequence, while GPT uses a uni-directional transformer. On the other hand, ELMo uses bi-directional LSTM to capture the bi-directional context while processing the input.

<!-- \begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{images/bert1.png}
    \caption{BERT \cite{devlin2018bert}}
    \label{fig:bert1}
\end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=\textwidth]{images/bert2.png}
%     \caption{BERT, GPT and ELMo \cite{devlin2018bert}}
%     \label{fig:bert2}
% \end{figure}
 -->

- **Other Resources**
    - Read the [BERT Paper](https://arxiv.org/pdf/1810.04805.pdf).
    - Have a look at [this amazing blog by Jay Alammar](http://jalammar.github.io/illustrated-bert/)).


## Further Steps

If you have made this far, then why not have some awesome resources for your further exploration. 

- I would recommend looking at these amazing repositories:
    - [NLP-Progress](https://nlpprogress.com/): An open-source repo that tracks the SOTA in several NLP tasks in multiple languages.
    - [Awesome NLP](https://github.com/keon/awesome-nlp/): Another open-source repo providing a single place for many NLP resources. 

- If you would like to study from some online courses, here are my top recommendations:
    - [fast.ai Course](https://www.fast.ai/2019/07/08/fastai-nlp/): A Code-First Introduction to NLP
    - [Stanford's CS224n](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/index.html): Natural Language Processing with Deep Learning
    - [Stanford's CS224U](http://web.stanford.edu/class/cs224u/): Natural Language Understanding

## Acknowledgement

- The blog's idea is highly inspired by [Sebestian Ruder's PhD thesis](https://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf). Especially the background chapter, where a quick overview of deep learning is provided before proceeding to the contributions of the thesis.

- I would like to thank [Dr Muneendra Ojha](https://www.iiitnr.ac.in/node/1614) and [Krutika Bapat](https://krutikabapat.github.io/) for reviewing the blog.

- I took the help of numerous online resources to get a better understanding of this field. Some are listed here if I have missed any blog/resource - sincere apologies for that. Please comment here or write an email, I'll properly acknowledge them.
    - [Sebestian Ruder's Blogs](https://ruder.io/)
    - [Christopher Olah's Blogs](https://colah.github.io/)
    - [Jay Alammar's Blogs](http://jalammar.github.io/)
    - [Stanford's CS224n Course](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/)
    - [Stanford's CS224u Course](http://web.stanford.edu/class/cs224u/)
    - [Analytics Vidhya Blog](https://www.analyticsvidhya.com/blog/)


## References

<a name="myfootnote1">[1]</a> Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S. & Dean, J. (2013), Distributed representations of words and phrases and their compositionality,in‘Advances in neural information processing systems’, pp.3111–3119.

<a name="myfootnote2">[2]</a> Pennington,J., Socher,R. & Manning,C. (2014), Glove: Global vectors for word representation, in ‘Proceed-ings of the 2014 conference on empirical methods in natural language processing (EMNLP)’, pp. 1532–1543.

<a name="myfootnote3">[3]</a> Bahdanau, D., Cho, K. & Bengio, Y. (2014), ‘Neural machine translation by jointly learning to align and translate’,arXiv preprint arXiv:1409.0473.

<a name="myfootnote4">[4]</a> Elman, J. L. (1990), ‘Finding structure in time’,Cognitive science14(2), 179–211.Graves, A., Jaitly, N. & Mohamed, A.-r. (2013), Hybrid speech recognition with deep bidirectional lstm, in ‘2013 IEEE workshop on automatic speech recognition and understanding’, IEEE, pp. 273–278.

<a name="myfootnote5">[5]</a> Bengio, Y., Simard, P., Frasconi, P. et al. (1994), ‘Learning long-term dependencies with gradient descent is difficult’, IEEE transactions on neural networks.

<a name="myfootnote6">[6]</a> Hochreiter, S. & Schmidhuber, J. (1997), ‘Long short-term memory’, Neural computation 9(8), 1735–1780.

<a name="myfootnote7">[7]</a> Cho, Kyunghyun, et al. "Learning phrase representations using RNN encoder-decoder for statistical machine translation." arXiv preprint arXiv:1406.1078 (2014).

<a name="myfootnote8">[8]</a> LeCun, Y., Bottou, L., Bengio, Y., Haffner, P. et al. (1998), ‘Gradient-based learning applied to document recognition’,Proceedings of the IEEE86(11), 2278–2324.

<a name="myfootnote9">[9]</a> Kim, Yoon. "Convolutional Neural Networks for Sentence Classification." Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2014.

<a name="myfootnote10">[10]</a> Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł. & Polosukhin, I.(2017),Attention is all you need, in‘Advances in neural information processing systems’, pp.5998–6008

<a name="myfootnote11">[11]</a> Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K. & Zettlemoyer, L. (2018), ‘Deep contextualized word representations’,arXiv preprint arXiv:1802.05365.

<a name="myfootnote12">[12]</a> Howard, J.&Ruder, S.(2018), ‘Universal language model fine-tuning for text classification’, arXiv preprintarXiv:1801.06146.

<a name="myfootnote13">[13]</a> Radford, A., Narasimhan, K., Salimans, T. & Sutskever, I. (2018), ‘Improving language un-derstanding by generative pre-training’.

<a name="myfootnote14">[14]</a> Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. (2018), ‘Bert: Pre-training of deep bidirectional transformers for language understanding’, arXiv preprint arXiv:1810.04805.
